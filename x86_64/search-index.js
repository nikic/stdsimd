var N=null,E="",T="t",U="u",searchIndex={};
var r_xe="std_detect";
var r_0a="Convert packed 16-bit integers from `a` and `b` to packed…",r_0b="Horizontal addition of adjacent pairs in the two packed…",r_0c="Store packed 64-bit integers from `a` into memory pointed…",r_0d="Counts the bits that are set.",r_1a="Return a mask of the most significant bit of each element…",r_1b="Horizontal subtraction of adjacent pairs in the two packed…",r_1c="Compare packed 8-bit integers in `a` and `b`, and return…",r_1d="Extracts bits in range [`start`, `start` + `length`) from…",r_2a="Return a new vector with the low element of `a` replaced…",r_2b="Convert packed 32-bit integers in `a` to packed…",r_2c="Multiply the low 32-bit integers from each packed 64-bit…",r_2d="Extracts bits of `a` specified by `control` into the least…",r_3a="Compute the bitwise OR of `a` and `b`.",r_3b="__m256i",r_3c="Multiply the low unsigned 32-bit integers from each packed…",r_3d="Bitwise logical `AND` of inverted `a` with `b`.",r_4a="Compare corresponding elements in `a` and `b` for…",r_4b="Shuffle single-precision (32-bit) floating-point elements…",r_4c="Multiply the packed 16-bit integers in `a` and `b`,…",r_4d="Extract lowest set isolated bit.",r_5a="Compare corresponding elements in `a` and `b` to see if…",r_5b="Shuffle double-precision (64-bit) floating-point elements…",r_5c="Multiply the packed 32-bit integers in `a` and `b`,…",r_5d="Get mask up to lowest set bit.",r_6a="Compare the lower element of `a` and `b` for…",r_6b="Broadcast a single-precision (32-bit) floating-point…",r_6c="Shuffle bytes from `a` according to the content of `b`.",r_6d="Resets the lowest set bit of `x`.",r_7a="Compare the lower element of `a` and `b` for equality.",r_7b="Load 256-bits (composed of 4 packed double-precision…",r_7c="Shift packed 16-bit integers in `a` left by `count` while…",r_7d="Unsigned multiply without affecting flags.",r_8a="Compare the lower element of `a` and `b` for less-than.",r_8b="Store 256-bits (composed of 4 packed double-precision…",r_8c="Shift packed 32-bit integers in `a` left by `count` while…",r_8d="Zero higher bits of `a` >= `index`.",r_9a="Compare the lower element of `a` and `b` for greater-than.",r_9b="Load 256-bits (composed of 8 packed single-precision…",r_9c="Shift packed 64-bit integers in `a` left by `count` while…",r_9d="Scatter contiguous low order bits of `a` to the result at…",r_Aa="Converts the lower 4 8-bit values of `a` into a 128-bit…",r_Ab="Compare packed 8-bit integers in `a` and `b` and return…",r_Ac="Add packed 8-bit integers in `a` and `b` using saturation.",r_Ad="Multiply the lower single-precision (32-bit)…",r_Ba="Converts a 64-bit vector of `i16`s into a 128-bit vector…",r_Bb="Compare packed unsigned 16-bit integers in `a` and `b`,…",r_Bc="Add packed 16-bit integers in `a` and `b` using saturation.",r_Bd="Counts the number of trailing least significant zero bits.",r_Ca="Conditionally copies the values from each 8-bit element in…",r_Cb="Compare packed 32-bit integers in `a` and `b`, and return…",r_Cc="Add packed unsigned 8-bit integers in `a` and `b` using…",r_Cd="Non-temporal store of `a.0` into `p`.",r_Da="Extracts 16-bit element from a 64-bit vector of `[4 x…",r_Db="Compare packed unsigned 32-bit integers in `a` and `b`,…",r_Dc="Add packed unsigned 16-bit integers in `a` and `b` using…",r_Dd="Clears all bits below the least significant zero bit of `x`.",r_Ea="Copies data from the 64-bit vector of `[4 x i16]` to the…",r_Eb="Convert packed 32-bit integers from `a` and `b` to packed…",r_Ec="Average packed unsigned 16-bit integers in `a` and `b`.",r_Ed="Sets all bits of `x` to 1 except for the least significant…",r_Fa="Takes the most significant bit from each 8-bit element in…",r_Fb="Zero extend packed unsigned 8-bit integers in `a` to…",r_Fc="Average packed unsigned 8-bit integers in `a` and `b`.",r_Fd="u64",r_Ga="Shuffles the 4 16-bit integers from a 64-bit integer…",r_Gb="Zero extend packed unsigned 16-bit integers in `a` to…",r_Gc="Blend packed 32-bit integers from `a` and `b` using…",r_Gd="Sets the least significant zero bit of `x` and clears all…",r_Ha="Convert the two lower packed single-precision (32-bit)…",r_Hb="Round the packed double-precision (64-bit) floating-point…",r_Hc="Broadcast the low packed 8-bit integer from `a` to all…",r_Hd="Sets the least significant zero bit of `x`.",r_Ia="Convert packed single-precision (32-bit) floating-point…",r_Ib="Round the packed single-precision (32-bit) floating-point…",r_Ic="Broadcast the low packed 32-bit integer from `a` to all…",r_Id="Sets all bits of `x` below the least significant one.",r_Ja="Perform a serializing operation on all load-from-memory…",r_Jb="Round the lower double-precision (64-bit) floating-point…",r_Jc="Broadcast the low packed 64-bit integer from `a` to all…",r_Jd="Clears least significant bit and sets all other bits.",r_Ka="__m128i",r_Kb="Round the lower single-precision (32-bit) floating-point…",r_Kc="Broadcast the low double-precision (64-bit) floating-point…",r_Kd="Clears all bits below the least significant zero of `x`…",r_La="Compare packed 16-bit integers in `a` and `b`, and return…",r_Lb="Tests whether the specified bits in a 128-bit integer…",r_Lc="Broadcast the low single-precision (32-bit) floating-point…",r_Ld="Sets all bits below the least significant one of `x` and…",r_Ma="Compare packed unsigned 8-bit integers in `a` and `b`, and…",r_Mb="Compare packed strings with implicit lengths in `a` and…",r_Mc="Broadcast the low packed 16-bit integer from a to all…",r_Md="Subtract packed 32-bit integers in `b` from packed 32-bit…",r_Na="Multiply the packed 16-bit integers in `a` and `b`.",r_Nb="Compare packed strings in `a` and `b` with lengths `la`…",r_Nc="Compare packed 32-bit integers in `a` and `b` for equality.",r_Nd="Compares whether each element of `a` is greater than the…",r_Oa="Subtract packed 8-bit integers in `b` from packed 8-bit…",r_Ob="Starting with the initial value in `crc`, return the…",r_Oc="Compare packed 16-bit integers in `a` and `b` for equality.",r_Od="Empty the MMX state, which marks the x87 FPU registers as…",r_Pa="Subtract packed 16-bit integers in `b` from packed 16-bit…",r_Pb="Add packed double-precision (64-bit) floating-point…",r_Pc="Compare packed 8-bit integers in `a` and `b` for equality.",r_Pd="Add unsigned 32-bit integers a and b with unsigned 8-bit…",r_Qa="Shift `a` left by `imm8` bytes while shifting in zeros.",r_Qb="__m256d",r_Qc="Compare packed 64-bit integers in `a` and `b` for…",r_Qd="Computes the absolute values of packed 32-bit integers in…",r_Ra="Shift `a` right by `imm8` bytes while shifting in zeros.",r_Rb="__m256",r_Rc="Compare packed 32-bit integers in `a` and `b` for…",r_Rd="__m512i",r_Sa="Shift packed 16-bit integers in `a` right by `imm8` while…",r_Sb="Compare packed double-precision (64-bit) floating-point…",r_Sc="Compare packed 16-bit integers in `a` and `b` for…",r_Sd="Compute the absolute value of packed 32-bit integers in…",r_Ta="Shift packed 16-bit integers in `a` right by `count` while…",r_Tb="Add packed single-precision (32-bit) floating-point…",r_Tc="Compare packed 8-bit integers in `a` and `b` for…",r_Td="__mmask16",r_Ua="Shift packed 32-bit integers in `a` right by `imm8` while…",r_Ub="Alternatively add and subtract packed double-precision…",r_Uc="Horizontally add adjacent pairs of 16-bit integers in `a`…",r_Ud="Saves the `x87` FPU, `MMX` technology, `XMM`, and `MXCSR`…",r_Va="Shift packed 32-bit integers in `a` right by `count` while…",r_Vb="Alternatively add and subtract packed single-precision…",r_Vc="Horizontally subtract adjacent pairs of 16-bit integers in…",r_Vd="Restores the `XMM`, `MMX`, `MXCSR`, and `x87` FPU…",r_Wa="__m128d",r_Wb="Subtract packed double-precision (64-bit) floating-point…",r_Wc="Return values from `slice` at offsets determined by…",r_Wd="Return a vector whose lowest element is `a` and all higher…",r_Xa="i16",r_Xb="Round packed double-precision (64-bit) floating point…",r_Xc="Load packed 32-bit integers from memory pointed by…",r_Xd="Return the lowest element of `a`.",r_Ya="Load 128-bits of integer data from memory into a new vector.",r_Yb="Round packed single-precision (32-bit) floating point…",r_Yc="Load packed 64-bit integers from memory pointed by…",r_Yd="Return `a` with its lower element replaced by `b` after…",r_Za="Store 128-bits of integer data from `a` into memory.",r_Zb="Return the square root of packed single-precision (32-bit)…",r_Zc="Store packed 32-bit integers from `a` into memory pointed…",r_Zd="Counts the leading most significant zero bits.",r_aa="core_arch",r_ab="Compare the lower element of `a` and `b` for not-equal.",r_ac="Store 256-bits (composed of 8 packed single-precision…",r_ad="Shift packed 16-bit integers in `a` left by `imm8` while…",r_ae="Gathers the bits of `x` specified by the `mask` into the…",r_ba="Reads the current value of the processor’s time-stamp…",r_bb="Convert packed double-precision (64-bit) floating-point…",r_bc="Load 256-bits of integer data from memory into result.…",r_bd="Shift packed 32-bit integers in `a` left by `imm8` while…",r_be="Return an integer with the reversed byte order of x",r_ca="cpuidresult",r_cb="Convert the lower double-precision (64-bit) floating-point…",r_cc="Store 256-bits of integer data from `a` into memory.…",r_cd="Shift packed 64-bit integers in `a` left by `imm8` while…",r_ce="Add unsigned 64-bit integers a and b with unsigned 8-bit…",r_da="Perform a full or partial save of the enabled processor…",r_db="Broadcast double-precision (64-bit) floating-point value a…",r_dc="Load packed double-precision (64-bit) floating-point…",r_dd="Shift 128-bit lanes in `a` left by `imm8` bytes while…",r_de="See `_mm_prefetch`.",r_ea="Perform a full or partial restore of the enabled processor…",r_eb="f64",r_ec="Store packed double-precision (64-bit) floating-point…",r_ed="Shift packed 32-bit integers in `a` left by the amount…",r_ee="String contains unsigned 16-bit characters",r_fa="__m128",r_fb="Set packed double-precision (64-bit) floating-point…",r_fc="Load packed single-precision (32-bit) floating-point…",r_fd="Shift packed 64-bit integers in `a` left by the amount…",r_fe="result",r_ga="Compare the first single-precision (32-bit) floating-point…",r_gb="Store 128-bits (composed of 2 packed double-precision…",r_gc="Store packed single-precision (32-bit) floating-point…",r_gd="Shift packed 32-bit integers in `a` right by the amount…",r_ge="self",r_ha="Compare packed single-precision (32-bit) floating-point…",r_hb="Store the lower double-precision (64-bit) floating-point…",r_hc="Duplicate odd-indexed single-precision (32-bit)…",r_hd="Shift 128-bit lanes in `a` right by `imm8` bytes while…",r_he="try_from",r_ia="Compare the lowest `f32` of both inputs for…",r_ib="Stores the lower 64 bits of a 128-bit vector of `[2 x…",r_ic="Duplicate even-indexed single-precision (32-bit)…",r_id="Shift packed 64-bit integers in `a` right by `count` while…",r_ie="try_into",r_ja="Compare each of the four floats in `a` to the…",r_jb="Load a double-precision (64-bit) floating-point element…",r_jc="Unpack and interleave double-precision (64-bit)…",r_jd="Shift packed 64-bit integers in `a` right by `imm8` while…",r_je="borrow",r_ka="Compare two 32-bit floats from the low-order bits of `a`…",r_kb="Load 128-bits (composed of 2 packed double-precision…",r_kc="Compute the bitwise AND of 256 bits (representing integer…",r_kd="Shift packed 64-bit integers in `a` right by the amount…",r_ke="borrow_mut",r_la="i32",r_lb="Constructs a 128-bit floating-point vector of `[2 x…",r_lc="Compute the bitwise AND of 256 bits (representing…",r_ld="Subtract packed unsigned 16-bit integers in `b` from…",r_le="type_id",r_ma="Convert the lowest 32 bit float in the input vector to a…",r_mb="Casts a 128-bit floating-point vector of `[2 x double]`…",r_mc="Compute the bitwise AND of 128 bits (representing…",r_md="Subtract packed unsigned 8-bit integers in `b` from packed…",r_me="typeid",r_na="f32",r_nb="Casts a 128-bit floating-point vector of `[4 x float]`…",r_nc="Set each bit of the returned mask based on the most…",r_nd="Unpack and interleave 8-bit integers from the high half of…",r_ne="from",r_oa="Construct a `__m128` from four floating point values…",r_ob="Casts a 128-bit integer vector into a 128-bit…",r_oc="i64",r_od="Unpack and interleave 8-bit integers from the low half of…",r_oe="into",r_pa="u32",r_pb="The resulting `__m128d` element is composed by the…",r_pc="Set packed single-precision (32-bit) floating-point…",r_pd="Unpack and interleave 16-bit integers from the high half…",r_pe="formatter",r_qa="Unpack and interleave single-precision (32-bit)…",r_qb="Converts the two double-precision floating-point elements…",r_qc="Set packed 8-bit integers in returned vector with the…",r_qd="Unpack and interleave 16-bit integers from the low half of…",r_qe="fmt",r_ra="See `_mm_setcsr`",r_rb="Horizontally add adjacent pairs of single-precision…",r_rc="Set packed 16-bit integers in returned vector with the…",r_rd="Unpack and interleave 32-bit integers from the high half…",r_re="bool",r_sa="__m64",r_sb="Horizontally add the adjacent pairs of values contained in…",r_sc="Set packed 32-bit integers in returned vector with the…",r_sd="Unpack and interleave 32-bit integers from the low half of…",r_se="ordering",r_ta="Compares the packed 16-bit signed integers of `a` and `b`…",r_tb="Horizontally subtract the adjacent pairs of values…",r_tc="Set packed 64-bit integers in returned vector with the…",r_td="Unpack and interleave 64-bit integers from the high half…",r_te="clone",r_ua="Compares the packed 8-bit signed integers of `a` and `b`…",r_ub="Horizontally subtracts the adjacent pairs of values…",r_uc="Set packed __m256 returned vector with the supplied values.",r_ud="Unpack and interleave 64-bit integers from the low half of…",r_ue="__m512",r_va="Multiplies packed 16-bit unsigned integer values and…",r_vb="Negate packed 8-bit integers in `a` when the corresponding…",r_vc="Set packed __m256d returned vector with the supplied values.",r_vd="Extract an 8-bit integer from `a`, selected with `imm8`.…",r_ve="__m512d",r_wa="Computes the rounded averages of the packed unsigned 8-bit…",r_wb="Negate packed 16-bit integers in `a` when the…",r_wc="Set packed __m256i returned vector with the supplied values.",r_wd="Returns the first element of the input vector of `[8 x…",r_we="CpuidResult",r_xa="Computes the rounded averages of the packed unsigned…",r_xb="Negate packed 32-bit integers in `a` when the…",r_xc="Add packed 32-bit integers in `a` and `b`.",r_xd="Multiply packed double-precision (64-bit) floating-point…",r_ya="Subtracts the corresponding 8-bit unsigned integer values…",r_yb="Blend packed double-precision (64-bit) floating-point…",r_yc="Add packed 16-bit integers in `a` and `b`.",r_yd="Multiply packed single-precision (32-bit) floating-point…",r_za="Converts two elements of a 64-bit vector of `[2 x i32]`…",r_zb="Blend packed single-precision (32-bit) floating-point…",r_zc="Add packed 8-bit integers in `a` and `b`.",r_zd="Multiply the lower double-precision (64-bit)…";
searchIndex[r_xe]={"doc":"Run-time feature detection for the Rust standard library.","i":[[14,"is_arm_feature_detected",r_xe,"Prevents compilation if `is_arm_feature_detected` is used…",N,N],[14,"is_aarch64_feature_detected",E,"Prevents compilation if `is_aarch64_feature_detected` is…",N,N],[14,"is_powerpc_feature_detected",E,"Prevents compilation if `is_powerpc_feature_detected` is…",N,N],[14,"is_powerpc64_feature_detected",E,"Prevents compilation if `is_powerpc64_feature_detected` is…",N,N],[14,"is_mips_feature_detected",E,"Prevents compilation if `is_mips_feature_detected` is used…",N,N],[14,"is_mips64_feature_detected",E,"Prevents compilation if `is_mips64_feature_detected` is…",N,N],[14,"is_x86_feature_detected",E,"A macro to test at runtime whether a CPU feature is…",N,N]],"p":[]};
searchIndex[r_aa]={"doc":"SIMD and vendor intrinsics module.","i":[[0,"x86_64",r_aa,"Platform-specific intrinsics for the `x86_64` platform.",N,N],[3,r_sa,"core_arch::x86_64","64-bit wide integer vector type, x86-specific",N,N],[3,r_Ka,E,"128-bit wide integer vector type, x86-specific",N,N],[3,r_fa,E,"128-bit wide set of four `f32` types, x86-specific",N,N],[3,r_Wa,E,"128-bit wide set of two `f64` types, x86-specific",N,N],[3,r_3b,E,"256-bit wide integer vector type, x86-specific",N,N],[3,r_Rb,E,"256-bit wide set of eight `f32` types, x86-specific",N,N],[3,r_Qb,E,"256-bit wide set of four `f64` types, x86-specific",N,N],[3,r_Rd,E,"512-bit wide integer vector type, x86-specific",N,N],[3,r_ue,E,"512-bit wide set of sixteen `f32` types, x86-specific",N,N],[3,r_ve,E,"512-bit wide set of eight `f64` types, x86-specific",N,N],[3,r_we,E,"Result of the `cpuid` instruction.",N,N],[12,"eax",E,"EAX register.",0,N],[12,"ebx",E,"EBX register.",0,N],[12,"ecx",E,"ECX register.",0,N],[12,"edx",E,"EDX register.",0,N],[5,"_fxsave",E,r_Ud,N,N],[5,"_fxrstor",E,r_Vd,N,N],[5,"_bswap",E,r_be,N,[[[r_la]],[r_la]]],[5,"_rdtsc",E,r_ba,N,[[],[r_oc]]],[5,"__rdtscp",E,r_ba,N,N],[5,"__cpuid_count",E,"Returns the result of the `cpuid` instruction for a given…",N,[[[r_pa],[r_pa]],[r_ca]]],[5,"__cpuid",E,"See `__cpuid_count`.",N,[[[r_pa]],[r_ca]]],[5,"has_cpuid",E,"Does the host support the `cpuid` instruction?",N,[[],[r_re]]],[5,"__get_cpuid_max",E,"Returns the highest-supported `leaf` (`EAX`) and sub-leaf…",N,N],[5,"_xsave",E,r_da,N,N],[5,"_xrstor",E,r_ea,N,N],[5,"_xsetbv",E,"Copy 64-bits from `val` to the extended control register…",N,[[[r_pa],[r_Fd]]]],[5,"_xgetbv",E,"Reads the contents of the extended control register `XCR`…",N,[[[r_pa]],[r_Fd]]],[5,"_xsaveopt",E,r_da,N,N],[5,"_xsavec",E,r_da,N,N],[5,"_xsaves",E,r_da,N,N],[5,"_xrstors",E,r_ea,N,N],[5,"_mm_add_ss",E,"Adds the first component of `a` and `b`, the other…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_add_ps",E,"Adds __m128 vectors.",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_sub_ss",E,"Subtracts the first component of `b` from `a`, the other…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_sub_ps",E,"Subtracts __m128 vectors.",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_mul_ss",E,"Multiplies the first component of `a` and `b`, the other…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_mul_ps",E,"Multiplies __m128 vectors.",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_div_ss",E,"Divides the first component of `b` by `a`, the other…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_div_ps",E,"Divides __m128 vectors.",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_sqrt_ss",E,"Return the square root of the first single-precision…",N,[[[r_fa]],[r_fa]]],[5,"_mm_sqrt_ps",E,r_Zb,N,[[[r_fa]],[r_fa]]],[5,"_mm_rcp_ss",E,"Return the approximate reciprocal of the first…",N,[[[r_fa]],[r_fa]]],[5,"_mm_rcp_ps",E,"Return the approximate reciprocal of packed…",N,[[[r_fa]],[r_fa]]],[5,"_mm_rsqrt_ss",E,"Return the approximate reciprocal square root of the fist…",N,[[[r_fa]],[r_fa]]],[5,"_mm_rsqrt_ps",E,"Return the approximate reciprocal square root of packed…",N,[[[r_fa]],[r_fa]]],[5,"_mm_min_ss",E,r_ga,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_min_ps",E,r_ha,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_max_ss",E,r_ga,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_max_ps",E,r_ha,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_and_ps",E,"Bitwise AND of packed single-precision (32-bit)…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_andnot_ps",E,"Bitwise AND-NOT of packed single-precision (32-bit)…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_or_ps",E,"Bitwise OR of packed single-precision (32-bit)…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_xor_ps",E,"Bitwise exclusive OR of packed single-precision (32-bit)…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpeq_ss",E,"Compare the lowest `f32` of both inputs for equality. The…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmplt_ss",E,"Compare the lowest `f32` of both inputs for less than. The…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmple_ss",E,"Compare the lowest `f32` of both inputs for less than or…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpgt_ss",E,"Compare the lowest `f32` of both inputs for greater than.…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpge_ss",E,"Compare the lowest `f32` of both inputs for greater than…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpneq_ss",E,"Compare the lowest `f32` of both inputs for inequality.…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpnlt_ss",E,"Compare the lowest `f32` of both inputs for not-less-than.…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpnle_ss",E,r_ia,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpngt_ss",E,r_ia,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpnge_ss",E,r_ia,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpord_ss",E,"Check if the lowest `f32` of both inputs are ordered. The…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpunord_ss",E,"Check if the lowest `f32` of both inputs are unordered.…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpeq_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmplt_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmple_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpgt_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpge_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpneq_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpnlt_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpnle_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpngt_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpnge_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpord_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_cmpunord_ps",E,r_ja,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_comieq_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_comilt_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_comile_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_comigt_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_comige_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_comineq_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_ucomieq_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_ucomilt_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_ucomile_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_ucomigt_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_ucomige_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_ucomineq_ss",E,r_ka,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_cvtss_si32",E,r_ma,N,[[[r_fa]],[r_la]]],[5,"_mm_cvt_ss2si",E,"Alias for `_mm_cvtss_si32`.",N,[[[r_fa]],[r_la]]],[5,"_mm_cvttss_si32",E,r_ma,N,[[[r_fa]],[r_la]]],[5,"_mm_cvtt_ss2si",E,"Alias for `_mm_cvttss_si32`.",N,[[[r_fa]],[r_la]]],[5,"_mm_cvtss_f32",E,"Extract the lowest 32 bit float from the input vector.",N,[[[r_fa]],[r_na]]],[5,"_mm_cvtsi32_ss",E,"Convert a 32 bit integer to a 32 bit float. The result…",N,[[[r_fa],[r_la]],[r_fa]]],[5,"_mm_cvt_si2ss",E,"Alias for `_mm_cvtsi32_ss`.",N,[[[r_fa],[r_la]],[r_fa]]],[5,"_mm_set_ss",E,"Construct a `__m128` with the lowest element set to `a`…",N,[[[r_na]],[r_fa]]],[5,"_mm_set1_ps",E,"Construct a `__m128` with all element set to `a`.",N,[[[r_na]],[r_fa]]],[5,"_mm_set_ps1",E,"Alias for `_mm_set1_ps`",N,[[[r_na]],[r_fa]]],[5,"_mm_set_ps",E,r_oa,N,[[[r_na],[r_na],[r_na],[r_na]],[r_fa]]],[5,"_mm_setr_ps",E,r_oa,N,[[[r_na],[r_na],[r_na],[r_na]],[r_fa]]],[5,"_mm_setzero_ps",E,"Construct a `__m128` with all elements initialized to zero.",N,[[],[r_fa]]],[5,"_MM_SHUFFLE",E,"A utility function for creating masks to use with Intel…",N,[[[r_pa],[r_pa],[r_pa],[r_pa]],[r_la]]],[5,"_mm_shuffle_ps",E,"Shuffle packed single-precision (32-bit) floating-point…",N,[[[r_fa],[r_fa],[r_pa]],[r_fa]]],[5,"_mm_unpackhi_ps",E,r_qa,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_unpacklo_ps",E,r_qa,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_movehl_ps",E,"Combine higher half of `a` and `b`. The highwe half of `b`…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_movelh_ps",E,"Combine lower half of `a` and `b`. The lower half of `b`…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_movemask_ps",E,r_1a,N,[[[r_fa]],[r_la]]],[5,"_mm_loadh_pi",E,"Set the upper two single-precision floating-point values…",N,N],[5,"_mm_loadl_pi",E,"Load two floats from `p` into the lower half of a…",N,N],[5,"_mm_load_ss",E,"Construct a `__m128` with the lowest element read from `p`…",N,N],[5,"_mm_load1_ps",E,"Construct a `__m128` by duplicating the value read from…",N,N],[5,"_mm_load_ps1",E,"Alias for `_mm_load1_ps`",N,N],[5,"_mm_load_ps",E,"Load four `f32` values from aligned memory into a…",N,N],[5,"_mm_loadu_ps",E,"Load four `f32` values from memory into a `__m128`. There…",N,N],[5,"_mm_loadr_ps",E,"Load four `f32` values from aligned memory into a `__m128`…",N,N],[5,"_mm_storeh_pi",E,"Store the upper half of `a` (64 bits) into memory.",N,N],[5,"_mm_storel_pi",E,"Store the lower half of `a` (64 bits) into memory.",N,N],[5,"_mm_store_ss",E,"Store the lowest 32 bit float of `a` into memory.",N,N],[5,"_mm_store1_ps",E,"Store the lowest 32 bit float of `a` repeated four times…",N,N],[5,"_mm_store_ps1",E,"Alias for `_mm_store1_ps`",N,N],[5,"_mm_store_ps",E,"Store four 32-bit floats into aligned memory.",N,N],[5,"_mm_storeu_ps",E,"Store four 32-bit floats into memory. There are no…",N,N],[5,"_mm_storer_ps",E,"Store four 32-bit floats into aligned memory in reverse…",N,N],[5,"_mm_move_ss",E,"Return a `__m128` with the first component from `b` and…",N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_sfence",E,"Perform a serializing operation on all store-to-memory…",N,[[]]],[5,"_mm_getcsr",E,"Get the unsigned 32-bit value of the MXCSR control and…",N,[[],[r_pa]]],[5,"_mm_setcsr",E,"Set the MXCSR register with the 32-bit unsigned integer…",N,[[[r_pa]]]],[5,"_MM_GET_EXCEPTION_MASK",E,r_ra,N,[[],[r_pa]]],[5,"_MM_GET_EXCEPTION_STATE",E,r_ra,N,[[],[r_pa]]],[5,"_MM_GET_FLUSH_ZERO_MODE",E,r_ra,N,[[],[r_pa]]],[5,"_MM_GET_ROUNDING_MODE",E,r_ra,N,[[],[r_pa]]],[5,"_MM_SET_EXCEPTION_MASK",E,r_ra,N,[[[r_pa]]]],[5,"_MM_SET_EXCEPTION_STATE",E,r_ra,N,[[[r_pa]]]],[5,"_MM_SET_FLUSH_ZERO_MODE",E,r_ra,N,[[[r_pa]]]],[5,"_MM_SET_ROUNDING_MODE",E,r_ra,N,[[[r_pa]]]],[5,"_mm_prefetch",E,"Fetch the cache line that contains address `p` using the…",N,N],[5,"_mm_undefined_ps",E,"Return vector of type __m128 with undefined elements.",N,[[],[r_fa]]],[5,"_MM_TRANSPOSE4_PS",E,"Transpose the 4x4 matrix formed by 4 rows of __m128 in…",N,[[[r_fa],[r_fa],[r_fa],[r_fa]]]],[5,"_mm_stream_ps",E,"Stores `a` into the memory at `mem_addr` using a…",N,N],[5,"_mm_stream_pi",E,"Store 64-bits of integer data from a into memory using a…",N,N],[5,"_mm_max_pi16",E,r_ta,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_pmaxsw",E,r_ta,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_max_pu8",E,r_ua,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_pmaxub",E,r_ua,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_min_pi16",E,r_ta,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_pminsw",E,r_ta,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_min_pu8",E,r_ua,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_pminub",E,r_ua,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_mulhi_pu16",E,r_va,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_mullo_pi16",E,"Multiplies packed 16-bit integer values and writes the…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_pmulhuw",E,r_va,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_avg_pu8",E,r_wa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_pavgb",E,r_wa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_avg_pu16",E,r_xa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_pavgw",E,r_xa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_sad_pu8",E,r_ya,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_psadbw",E,r_ya,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_cvtpi32_ps",E,r_za,N,[[[r_fa],[r_sa]],[r_fa]]],[5,"_mm_cvt_pi2ps",E,r_za,N,[[[r_fa],[r_sa]],[r_fa]]],[5,"_mm_cvtpi8_ps",E,r_Aa,N,[[[r_sa]],[r_fa]]],[5,"_mm_cvtpu8_ps",E,r_Aa,N,[[[r_sa]],[r_fa]]],[5,"_mm_cvtpi16_ps",E,r_Ba,N,[[[r_sa]],[r_fa]]],[5,"_mm_cvtpu16_ps",E,r_Ba,N,[[[r_sa]],[r_fa]]],[5,"_mm_cvtpi32x2_ps",E,"Converts the two 32-bit signed integer values from each…",N,[[[r_sa],[r_sa]],[r_fa]]],[5,"_mm_maskmove_si64",E,r_Ca,N,N],[5,"_m_maskmovq",E,r_Ca,N,N],[5,"_mm_extract_pi16",E,r_Da,N,[[[r_sa],[r_la]],[r_la]]],[5,"_m_pextrw",E,r_Da,N,[[[r_sa],[r_la]],[r_la]]],[5,"_mm_insert_pi16",E,r_Ea,N,[[[r_sa],[r_la],[r_la]],[r_sa]]],[5,"_m_pinsrw",E,r_Ea,N,[[[r_sa],[r_la],[r_la]],[r_sa]]],[5,"_mm_movemask_pi8",E,r_Fa,N,[[[r_sa]],[r_la]]],[5,"_m_pmovmskb",E,r_Fa,N,[[[r_sa]],[r_la]]],[5,"_mm_shuffle_pi16",E,r_Ga,N,[[[r_sa],[r_la]],[r_sa]]],[5,"_m_pshufw",E,r_Ga,N,[[[r_sa],[r_la]],[r_sa]]],[5,"_mm_cvttps_pi32",E,r_Ha,N,[[[r_fa]],[r_sa]]],[5,"_mm_cvtt_ps2pi",E,r_Ha,N,[[[r_fa]],[r_sa]]],[5,"_mm_cvtps_pi32",E,r_Ha,N,[[[r_fa]],[r_sa]]],[5,"_mm_cvt_ps2pi",E,r_Ha,N,[[[r_fa]],[r_sa]]],[5,"_mm_cvtps_pi16",E,r_Ia,N,[[[r_fa]],[r_sa]]],[5,"_mm_cvtps_pi8",E,r_Ia,N,[[[r_fa]],[r_sa]]],[5,"_mm_pause",E,"Provide a hint to the processor that the code sequence is…",N,[[]]],[5,"_mm_clflush",E,"Invalidate and flush the cache line that contains `p` from…",N,N],[5,"_mm_lfence",E,r_Ja,N,[[]]],[5,"_mm_mfence",E,r_Ja,N,[[]]],[5,"_mm_add_epi8",E,r_zc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_add_epi16",E,r_yc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_add_epi32",E,r_xc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_add_epi64",E,"Add packed 64-bit integers in `a` and \"b`.",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_adds_epi8",E,r_Ac,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_adds_epi16",E,r_Bc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_adds_epu8",E,r_Cc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_adds_epu16",E,r_Dc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_avg_epu8",E,r_Fc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_avg_epu16",E,r_Ec,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_madd_epi16",E,"Multiply and then horizontally add signed 16 bit integers…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_max_epi16",E,r_La,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_max_epu8",E,r_Ma,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_min_epi16",E,r_La,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_min_epu8",E,r_Ma,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_mulhi_epi16",E,r_Na,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_mulhi_epu16",E,"Multiply the packed unsigned 16-bit integers in `a` and `b`.",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_mullo_epi16",E,r_Na,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_mul_epu32",E,r_3c,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sad_epu8",E,"Sum the absolute differences of packed unsigned 8-bit…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sub_epi8",E,r_Oa,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sub_epi16",E,r_Pa,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sub_epi32",E,r_Md,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sub_epi64",E,"Subtract packed 64-bit integers in `b` from packed 64-bit…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_subs_epi8",E,r_Oa,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_subs_epi16",E,r_Pa,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_subs_epu8",E,r_md,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_subs_epu16",E,r_ld,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_slli_si128",E,r_Qa,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_bslli_si128",E,r_Qa,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_bsrli_si128",E,r_Ra,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_slli_epi16",E,r_ad,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_sll_epi16",E,r_7c,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_slli_epi32",E,r_bd,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_sll_epi32",E,r_8c,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_slli_epi64",E,r_cd,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_sll_epi64",E,r_9c,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_srai_epi16",E,r_Sa,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_sra_epi16",E,r_Ta,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_srai_epi32",E,r_Ua,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_sra_epi32",E,r_Va,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_srli_si128",E,r_Ra,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_srli_epi16",E,r_Sa,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_srl_epi16",E,r_Ta,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_srli_epi32",E,r_Ua,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_srl_epi32",E,r_Va,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_srli_epi64",E,r_jd,N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_srl_epi64",E,r_id,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_and_si128",E,"Compute the bitwise AND of 128 bits (representing integer…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_andnot_si128",E,"Compute the bitwise NOT of 128 bits (representing integer…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_or_si128",E,"Compute the bitwise OR of 128 bits (representing integer…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_xor_si128",E,"Compute the bitwise XOR of 128 bits (representing integer…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cmpeq_epi8",E,r_Pc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cmpeq_epi16",E,r_Oc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cmpeq_epi32",E,r_Nc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cmpgt_epi8",E,r_Tc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cmpgt_epi16",E,r_Sc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cmpgt_epi32",E,r_Rc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cmplt_epi8",E,"Compare packed 8-bit integers in `a` and `b` for less-than.",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cmplt_epi16",E,"Compare packed 16-bit integers in `a` and `b` for less-than.",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cmplt_epi32",E,"Compare packed 32-bit integers in `a` and `b` for less-than.",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cvtepi32_pd",E,"Convert the lower two packed 32-bit integers in `a` to…",N,[[[r_Ka]],[r_Wa]]],[5,"_mm_cvtsi32_sd",E,r_Yd,N,[[[r_Wa],[r_la]],[r_Wa]]],[5,"_mm_cvtepi32_ps",E,r_2b,N,[[[r_Ka]],[r_fa]]],[5,"_mm_cvtps_epi32",E,r_Ia,N,[[[r_fa]],[r_Ka]]],[5,"_mm_cvtsi32_si128",E,r_Wd,N,[[[r_la]],[r_Ka]]],[5,"_mm_cvtsi128_si32",E,r_Xd,N,[[[r_Ka]],[r_la]]],[5,"_mm_set_epi64x",E,"Set packed 64-bit integers with the supplied values, from…",N,[[[r_oc],[r_oc]],[r_Ka]]],[5,"_mm_set_epi32",E,"Set packed 32-bit integers with the supplied values.",N,[[[r_la],[r_la],[r_la],[r_la]],[r_Ka]]],[5,"_mm_set_epi16",E,"Set packed 16-bit integers with the supplied values.",N,[[[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa]],[r_Ka]]],[5,"_mm_set_epi8",E,"Set packed 8-bit integers with the supplied values.",N,[[["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"]],[r_Ka]]],[5,"_mm_set1_epi64x",E,"Broadcast 64-bit integer `a` to all elements.",N,[[[r_oc]],[r_Ka]]],[5,"_mm_set1_epi32",E,"Broadcast 32-bit integer `a` to all elements.",N,[[[r_la]],[r_Ka]]],[5,"_mm_set1_epi16",E,"Broadcast 16-bit integer `a` to all elements.",N,[[[r_Xa]],[r_Ka]]],[5,"_mm_set1_epi8",E,"Broadcast 8-bit integer `a` to all elements.",N,[[["i8"]],[r_Ka]]],[5,"_mm_setr_epi32",E,"Set packed 32-bit integers with the supplied values in…",N,[[[r_la],[r_la],[r_la],[r_la]],[r_Ka]]],[5,"_mm_setr_epi16",E,"Set packed 16-bit integers with the supplied values in…",N,[[[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa]],[r_Ka]]],[5,"_mm_setr_epi8",E,"Set packed 8-bit integers with the supplied values in…",N,[[["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"]],[r_Ka]]],[5,"_mm_setzero_si128",E,"Returns a vector with all elements set to zero.",N,[[],[r_Ka]]],[5,"_mm_loadl_epi64",E,"Load 64-bit integer from memory into first element of…",N,N],[5,"_mm_load_si128",E,r_Ya,N,N],[5,"_mm_loadu_si128",E,r_Ya,N,N],[5,"_mm_maskmoveu_si128",E,"Conditionally store 8-bit integer elements from `a` into…",N,N],[5,"_mm_store_si128",E,r_Za,N,N],[5,"_mm_storeu_si128",E,r_Za,N,N],[5,"_mm_storel_epi64",E,"Store the lower 64-bit integer `a` to a memory location.",N,N],[5,"_mm_stream_si128",E,"Stores a 128-bit integer vector to a 128-bit aligned…",N,N],[5,"_mm_stream_si32",E,"Stores a 32-bit integer value in the specified memory…",N,N],[5,"_mm_move_epi64",E,"Return a vector where the low element is extracted from…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_packs_epi16",E,r_0a,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_packs_epi32",E,r_Eb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_packus_epi16",E,r_0a,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_extract_epi16",E,"Return the `imm8` element of `a`.",N,[[[r_Ka],[r_la]],[r_la]]],[5,"_mm_insert_epi16",E,"Return a new vector where the `imm8` element of `a` is…",N,[[[r_Ka],[r_la],[r_la]],[r_Ka]]],[5,"_mm_movemask_epi8",E,r_1a,N,[[[r_Ka]],[r_la]]],[5,"_mm_shuffle_epi32",E,"Shuffle 32-bit integers in `a` using the control in `imm8`.",N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_shufflehi_epi16",E,"Shuffle 16-bit integers in the high 64 bits of `a` using…",N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_shufflelo_epi16",E,"Shuffle 16-bit integers in the low 64 bits of `a` using…",N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_unpackhi_epi8",E,r_nd,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_unpackhi_epi16",E,r_pd,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_unpackhi_epi32",E,r_rd,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_unpackhi_epi64",E,r_td,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_unpacklo_epi8",E,r_od,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_unpacklo_epi16",E,r_qd,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_unpacklo_epi32",E,r_sd,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_unpacklo_epi64",E,r_ud,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_add_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_add_pd",E,r_Pb,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_div_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_div_pd",E,"Divide packed double-precision (64-bit) floating-point…",N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_max_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_max_pd",E,"Return a new vector with the maximum values from…",N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_min_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_min_pd",E,"Return a new vector with the minimum values from…",N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_mul_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_mul_pd",E,r_xd,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_sqrt_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_sqrt_pd",E,"Return a new vector with the square root of each of the…",N,[[[r_Wa]],[r_Wa]]],[5,"_mm_sub_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_sub_pd",E,r_Wb,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_and_pd",E,"Compute the bitwise AND of packed double-precision…",N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_andnot_pd",E,"Compute the bitwise NOT of `a` and then AND with `b`.",N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_or_pd",E,r_3a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_xor_pd",E,r_3a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpeq_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmplt_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmple_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpgt_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpge_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpord_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpunord_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpneq_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpnlt_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpnle_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpngt_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpnge_sd",E,r_2a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpeq_pd",E,"Compare corresponding elements in `a` and `b` for equality.",N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmplt_pd",E,"Compare corresponding elements in `a` and `b` for less-than.",N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmple_pd",E,r_4a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpgt_pd",E,r_4a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpge_pd",E,r_4a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpord_pd",E,r_5a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpunord_pd",E,r_5a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpneq_pd",E,"Compare corresponding elements in `a` and `b` for not-equal.",N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpnlt_pd",E,r_4a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpnle_pd",E,r_4a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpngt_pd",E,r_4a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_cmpnge_pd",E,r_4a,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_comieq_sd",E,r_7a,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_comilt_sd",E,r_8a,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_comile_sd",E,r_6a,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_comigt_sd",E,r_9a,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_comige_sd",E,r_6a,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_comineq_sd",E,r_ab,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_ucomieq_sd",E,r_7a,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_ucomilt_sd",E,r_8a,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_ucomile_sd",E,r_6a,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_ucomigt_sd",E,r_9a,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_ucomige_sd",E,r_6a,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_ucomineq_sd",E,r_ab,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_cvtpd_ps",E,r_bb,N,[[[r_Wa]],[r_fa]]],[5,"_mm_cvtps_pd",E,r_Ia,N,[[[r_fa]],[r_Wa]]],[5,"_mm_cvtpd_epi32",E,r_bb,N,[[[r_Wa]],[r_Ka]]],[5,"_mm_cvtsd_si32",E,r_cb,N,[[[r_Wa]],[r_la]]],[5,"_mm_cvtsd_ss",E,r_cb,N,[[[r_fa],[r_Wa]],[r_fa]]],[5,"_mm_cvtsd_f64",E,"Return the lower double-precision (64-bit) floating-point…",N,[[[r_Wa]],[r_eb]]],[5,"_mm_cvtss_sd",E,"Convert the lower single-precision (32-bit) floating-point…",N,[[[r_Wa],[r_fa]],[r_Wa]]],[5,"_mm_cvttpd_epi32",E,r_bb,N,[[[r_Wa]],[r_Ka]]],[5,"_mm_cvttsd_si32",E,r_cb,N,[[[r_Wa]],[r_la]]],[5,"_mm_cvttps_epi32",E,r_Ia,N,[[[r_fa]],[r_Ka]]],[5,"_mm_set_sd",E,"Copy double-precision (64-bit) floating-point element `a`…",N,[[[r_eb]],[r_Wa]]],[5,"_mm_set1_pd",E,r_db,N,[[[r_eb]],[r_Wa]]],[5,"_mm_set_pd1",E,r_db,N,[[[r_eb]],[r_Wa]]],[5,"_mm_set_pd",E,r_fb,N,[[[r_eb],[r_eb]],[r_Wa]]],[5,"_mm_setr_pd",E,r_fb,N,[[[r_eb],[r_eb]],[r_Wa]]],[5,"_mm_setzero_pd",E,"Returns packed double-precision (64-bit) floating-point…",N,[[],[r_Wa]]],[5,"_mm_movemask_pd",E,r_1a,N,[[[r_Wa]],[r_la]]],[5,"_mm_load_pd",E,r_kb,N,N],[5,"_mm_load_sd",E,"Loads a 64-bit double-precision value to the low element…",N,N],[5,"_mm_loadh_pd",E,"Loads a double-precision value into the high-order bits of…",N,N],[5,"_mm_loadl_pd",E,"Loads a double-precision value into the low-order bits of…",N,N],[5,"_mm_stream_pd",E,"Stores a 128-bit floating point vector of `[2 x double]`…",N,N],[5,"_mm_store_sd",E,r_ib,N,N],[5,"_mm_store_pd",E,r_gb,N,N],[5,"_mm_storeu_pd",E,r_gb,N,N],[5,"_mm_store1_pd",E,r_hb,N,N],[5,"_mm_store_pd1",E,r_hb,N,N],[5,"_mm_storer_pd",E,"Store 2 double-precision (64-bit) floating-point elements…",N,N],[5,"_mm_storeh_pd",E,"Stores the upper 64 bits of a 128-bit vector of `[2 x…",N,N],[5,"_mm_storel_pd",E,r_ib,N,N],[5,"_mm_load1_pd",E,r_jb,N,N],[5,"_mm_load_pd1",E,r_jb,N,N],[5,"_mm_loadr_pd",E,"Load 2 double-precision (64-bit) floating-point elements…",N,N],[5,"_mm_loadu_pd",E,r_kb,N,N],[5,"_mm_shuffle_pd",E,r_lb,N,[[[r_Wa],[r_Wa],[r_la]],[r_Wa]]],[5,"_mm_move_sd",E,r_lb,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_castpd_ps",E,r_mb,N,[[[r_Wa]],[r_fa]]],[5,"_mm_castpd_si128",E,r_mb,N,[[[r_Wa]],[r_Ka]]],[5,"_mm_castps_pd",E,r_nb,N,[[[r_fa]],[r_Wa]]],[5,"_mm_castps_si128",E,r_nb,N,[[[r_fa]],[r_Ka]]],[5,"_mm_castsi128_pd",E,r_ob,N,[[[r_Ka]],[r_Wa]]],[5,"_mm_castsi128_ps",E,r_ob,N,[[[r_Ka]],[r_fa]]],[5,"_mm_undefined_pd",E,"Return vector of type __m128d with undefined elements.",N,[[],[r_Wa]]],[5,"_mm_undefined_si128",E,"Return vector of type __m128i with undefined elements.",N,[[],[r_Ka]]],[5,"_mm_unpackhi_pd",E,r_pb,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_unpacklo_pd",E,r_pb,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_add_si64",E,"Adds two signed or unsigned 64-bit integer values,…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_mul_su32",E,"Multiplies 32-bit unsigned integer values contained in the…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_sub_si64",E,"Subtracts signed or unsigned 64-bit integer values and…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_cvtpi32_pd",E,"Converts the two signed 32-bit integer elements of a…",N,[[[r_sa]],[r_Wa]]],[5,"_mm_set_epi64",E,"Initializes both 64-bit values in a 128-bit vector of `[2…",N,[[[r_sa],[r_sa]],[r_Ka]]],[5,"_mm_set1_epi64",E,"Initializes both values in a 128-bit vector of `[2 x i64]`…",N,[[[r_sa]],[r_Ka]]],[5,"_mm_setr_epi64",E,"Constructs a 128-bit integer vector, initialized in…",N,[[[r_sa],[r_sa]],[r_Ka]]],[5,"_mm_movepi64_pi64",E,"Returns the lower 64 bits of a 128-bit integer vector as a…",N,[[[r_Ka]],[r_sa]]],[5,"_mm_movpi64_epi64",E,"Moves the 64-bit operand to a 128-bit integer vector,…",N,[[[r_sa]],[r_Ka]]],[5,"_mm_cvtpd_pi32",E,r_qb,N,[[[r_Wa]],[r_sa]]],[5,"_mm_cvttpd_pi32",E,r_qb,N,[[[r_Wa]],[r_sa]]],[5,"_mm_addsub_ps",E,r_Vb,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_addsub_pd",E,r_Ub,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_hadd_pd",E,"Horizontally add adjacent pairs of double-precision…",N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_hadd_ps",E,r_rb,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_hsub_pd",E,"Horizontally subtract adjacent pairs of double-precision…",N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_hsub_ps",E,r_rb,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_lddqu_si128",E,"Load 128-bits of integer data from unaligned memory. This…",N,N],[5,"_mm_movedup_pd",E,"Duplicate the low double-precision (64-bit) floating-point…",N,[[[r_Wa]],[r_Wa]]],[5,"_mm_loaddup_pd",E,r_jb,N,N],[5,"_mm_movehdup_ps",E,r_hc,N,[[[r_fa]],[r_fa]]],[5,"_mm_moveldup_ps",E,r_ic,N,[[[r_fa]],[r_fa]]],[5,"_mm_abs_epi8",E,"Compute the absolute value of packed 8-bit signed integers…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_abs_epi16",E,"Compute the absolute value of each of the packed 16-bit…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_abs_epi32",E,"Compute the absolute value of each of the packed 32-bit…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_shuffle_epi8",E,r_6c,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_alignr_epi8",E,"Concatenate 16-byte blocks in `a` and `b` into a 32-byte…",N,[[[r_Ka],[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_hadd_epi16",E,r_sb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_hadds_epi16",E,r_sb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_hadd_epi32",E,r_sb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_hsub_epi16",E,r_tb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_hsubs_epi16",E,r_tb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_hsub_epi32",E,r_tb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_maddubs_epi16",E,"Multiply corresponding pairs of packed 8-bit unsigned…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_mulhrs_epi16",E,"Multiply packed 16-bit signed integer values, truncate the…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sign_epi8",E,r_vb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sign_epi16",E,r_wb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sign_epi32",E,r_xb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_abs_pi8",E,"Compute the absolute value of packed 8-bit integers in `a`…",N,[[[r_sa]],[r_sa]]],[5,"_mm_abs_pi16",E,"Compute the absolute value of packed 8-bit integers in…",N,[[[r_sa]],[r_sa]]],[5,"_mm_abs_pi32",E,r_Sd,N,[[[r_sa]],[r_sa]]],[5,"_mm_shuffle_pi8",E,"Shuffle packed 8-bit integers in `a` according to shuffle…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_alignr_pi8",E,"Concatenates the two 64-bit integer vector operands, and…",N,[[[r_sa],[r_sa],[r_la]],[r_sa]]],[5,"_mm_hadd_pi16",E,r_sb,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_hadd_pi32",E,r_sb,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_hadds_pi16",E,r_sb,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_hsub_pi16",E,r_ub,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_hsub_pi32",E,r_ub,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_hsubs_pi16",E,r_ub,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_maddubs_pi16",E,"Multiplies corresponding pairs of packed 8-bit unsigned…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_mulhrs_pi16",E,"Multiplies packed 16-bit signed integer values, truncates…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_sign_pi8",E,r_vb,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_sign_pi16",E,r_wb,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_sign_pi32",E,r_xb,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_blendv_epi8",E,"Blend packed 8-bit integers from `a` and `b` using `mask`",N,[[[r_Ka],[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_blend_epi16",E,"Blend packed 16-bit integers from `a` and `b` using the…",N,[[[r_Ka],[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_blendv_pd",E,r_yb,N,[[[r_Wa],[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_blendv_ps",E,r_zb,N,[[[r_fa],[r_fa],[r_fa]],[r_fa]]],[5,"_mm_blend_pd",E,r_yb,N,[[[r_Wa],[r_Wa],[r_la]],[r_Wa]]],[5,"_mm_blend_ps",E,r_zb,N,[[[r_fa],[r_fa],[r_la]],[r_fa]]],[5,"_mm_extract_ps",E,"Extract a single-precision (32-bit) floating-point element…",N,[[[r_fa],[r_la]],[r_la]]],[5,"_mm_extract_epi8",E,r_vd,N,[[[r_Ka],[r_la]],[r_la]]],[5,"_mm_extract_epi32",E,"Extract an 32-bit integer from `a` selected with `imm8`",N,[[[r_Ka],[r_la]],[r_la]]],[5,"_mm_insert_ps",E,"Select a single value in `a` to store at some position in…",N,[[[r_fa],[r_fa],[r_la]],[r_fa]]],[5,"_mm_insert_epi8",E,"Return a copy of `a` with the 8-bit integer from `i`…",N,[[[r_Ka],[r_la],[r_la]],[r_Ka]]],[5,"_mm_insert_epi32",E,"Return a copy of `a` with the 32-bit integer from `i`…",N,[[[r_Ka],[r_la],[r_la]],[r_Ka]]],[5,"_mm_max_epi8",E,r_Ab,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_max_epu16",E,r_Bb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_max_epi32",E,r_Cb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_max_epu32",E,r_Db,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_min_epi8",E,r_Ab,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_min_epu16",E,r_Bb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_min_epi32",E,r_Cb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_min_epu32",E,r_Db,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_packus_epi32",E,r_Eb,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cmpeq_epi64",E,"Compare packed 64-bit integers in `a` and `b` for equality",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_cvtepi8_epi16",E,"Sign extend packed 8-bit integers in `a` to packed 16-bit…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_cvtepi8_epi32",E,"Sign extend packed 8-bit integers in `a` to packed 32-bit…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_cvtepi8_epi64",E,"Sign extend packed 8-bit integers in the low 8 bytes of…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_cvtepi16_epi32",E,"Sign extend packed 16-bit integers in `a` to packed 32-bit…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_cvtepi16_epi64",E,"Sign extend packed 16-bit integers in `a` to packed 64-bit…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_cvtepi32_epi64",E,"Sign extend packed 32-bit integers in `a` to packed 64-bit…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_cvtepu8_epi16",E,r_Fb,N,[[[r_Ka]],[r_Ka]]],[5,"_mm_cvtepu8_epi32",E,r_Fb,N,[[[r_Ka]],[r_Ka]]],[5,"_mm_cvtepu8_epi64",E,r_Fb,N,[[[r_Ka]],[r_Ka]]],[5,"_mm_cvtepu16_epi32",E,r_Gb,N,[[[r_Ka]],[r_Ka]]],[5,"_mm_cvtepu16_epi64",E,r_Gb,N,[[[r_Ka]],[r_Ka]]],[5,"_mm_cvtepu32_epi64",E,"Zero extend packed unsigned 32-bit integers in `a` to…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_dp_pd",E,"Returns the dot product of two __m128d vectors.",N,[[[r_Wa],[r_Wa],[r_la]],[r_Wa]]],[5,"_mm_dp_ps",E,"Returns the dot product of two __m128 vectors.",N,[[[r_fa],[r_fa],[r_la]],[r_fa]]],[5,"_mm_floor_pd",E,r_Hb,N,[[[r_Wa]],[r_Wa]]],[5,"_mm_floor_ps",E,r_Ib,N,[[[r_fa]],[r_fa]]],[5,"_mm_floor_sd",E,r_Jb,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_floor_ss",E,r_Kb,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_ceil_pd",E,r_Hb,N,[[[r_Wa]],[r_Wa]]],[5,"_mm_ceil_ps",E,r_Ib,N,[[[r_fa]],[r_fa]]],[5,"_mm_ceil_sd",E,r_Jb,N,[[[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_ceil_ss",E,r_Kb,N,[[[r_fa],[r_fa]],[r_fa]]],[5,"_mm_round_pd",E,r_Hb,N,[[[r_Wa],[r_la]],[r_Wa]]],[5,"_mm_round_ps",E,r_Ib,N,[[[r_fa],[r_la]],[r_fa]]],[5,"_mm_round_sd",E,r_Jb,N,[[[r_Wa],[r_Wa],[r_la]],[r_Wa]]],[5,"_mm_round_ss",E,r_Kb,N,[[[r_fa],[r_fa],[r_la]],[r_fa]]],[5,"_mm_minpos_epu16",E,"Finds the minimum unsigned 16-bit element in the 128-bit…",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_mul_epi32",E,r_2c,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_mullo_epi32",E,r_5c,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_mpsadbw_epu8",E,"Subtracts 8-bit unsigned integer values and computes the…",N,[[[r_Ka],[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_testz_si128",E,r_Lb,N,[[[r_Ka],[r_Ka]],[r_la]]],[5,"_mm_testc_si128",E,r_Lb,N,[[[r_Ka],[r_Ka]],[r_la]]],[5,"_mm_testnzc_si128",E,r_Lb,N,[[[r_Ka],[r_Ka]],[r_la]]],[5,"_mm_test_all_zeros",E,r_Lb,N,[[[r_Ka],[r_Ka]],[r_la]]],[5,"_mm_test_all_ones",E,"Tests whether the specified bits in `a` 128-bit integer…",N,[[[r_Ka]],[r_la]]],[5,"_mm_test_mix_ones_zeros",E,r_Lb,N,[[[r_Ka],[r_Ka]],[r_la]]],[5,"_mm_cmpistrm",E,r_Mb,N,[[[r_Ka],[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_cmpistri",E,r_Mb,N,[[[r_Ka],[r_Ka],[r_la]],[r_la]]],[5,"_mm_cmpistrz",E,r_Mb,N,[[[r_Ka],[r_Ka],[r_la]],[r_la]]],[5,"_mm_cmpistrc",E,r_Mb,N,[[[r_Ka],[r_Ka],[r_la]],[r_la]]],[5,"_mm_cmpistrs",E,r_Mb,N,[[[r_Ka],[r_Ka],[r_la]],[r_la]]],[5,"_mm_cmpistro",E,r_Mb,N,[[[r_Ka],[r_Ka],[r_la]],[r_la]]],[5,"_mm_cmpistra",E,r_Mb,N,[[[r_Ka],[r_Ka],[r_la]],[r_la]]],[5,"_mm_cmpestrm",E,r_Nb,N,[[[r_Ka],[r_la],[r_Ka],[r_la],[r_la]],[r_Ka]]],[5,"_mm_cmpestri",E,"Compare packed strings `a` and `b` with lengths `la` and…",N,[[[r_Ka],[r_la],[r_Ka],[r_la],[r_la]],[r_la]]],[5,"_mm_cmpestrz",E,r_Nb,N,[[[r_Ka],[r_la],[r_Ka],[r_la],[r_la]],[r_la]]],[5,"_mm_cmpestrc",E,r_Nb,N,[[[r_Ka],[r_la],[r_Ka],[r_la],[r_la]],[r_la]]],[5,"_mm_cmpestrs",E,r_Nb,N,[[[r_Ka],[r_la],[r_Ka],[r_la],[r_la]],[r_la]]],[5,"_mm_cmpestro",E,r_Nb,N,[[[r_Ka],[r_la],[r_Ka],[r_la],[r_la]],[r_la]]],[5,"_mm_cmpestra",E,r_Nb,N,[[[r_Ka],[r_la],[r_Ka],[r_la],[r_la]],[r_la]]],[5,"_mm_crc32_u8",E,r_Ob,N,[[[r_pa],["u8"]],[r_pa]]],[5,"_mm_crc32_u16",E,r_Ob,N,[[[r_pa],["u16"]],[r_pa]]],[5,"_mm_crc32_u32",E,r_Ob,N,[[[r_pa],[r_pa]],[r_pa]]],[5,"_mm_cmpgt_epi64",E,r_Qc,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm256_add_pd",E,r_Pb,N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_add_ps",E,r_Tb,N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_and_pd",E,"Compute the bitwise AND of a packed double-precision…",N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_and_ps",E,"Compute the bitwise AND of packed single-precision…",N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_or_pd",E,"Compute the bitwise OR packed double-precision (64-bit)…",N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_or_ps",E,"Compute the bitwise OR packed single-precision (32-bit)…",N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_shuffle_pd",E,r_5b,N,[[[r_Qb],[r_Qb],[r_la]],[r_Qb]]],[5,"_mm256_shuffle_ps",E,r_4b,N,[[[r_Rb],[r_Rb],[r_la]],[r_Rb]]],[5,"_mm256_andnot_pd",E,"Compute the bitwise NOT of packed double-precision…",N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_andnot_ps",E,"Compute the bitwise NOT of packed single-precision…",N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_max_pd",E,r_Sb,N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_max_ps",E,r_ha,N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_min_pd",E,r_Sb,N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_min_ps",E,r_ha,N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_mul_pd",E,r_Pb,N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_mul_ps",E,r_Tb,N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_addsub_pd",E,r_Ub,N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_addsub_ps",E,r_Vb,N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_sub_pd",E,r_Wb,N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_sub_ps",E,"Subtract packed single-precision (32-bit) floating-point…",N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_div_ps",E,"Compute the division of each of the 8 packed 32-bit…",N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_div_pd",E,"Compute the division of each of the 4 packed 64-bit…",N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_round_pd",E,r_Xb,N,[[[r_Qb],[r_la]],[r_Qb]]],[5,"_mm256_ceil_pd",E,r_Xb,N,[[[r_Qb]],[r_Qb]]],[5,"_mm256_floor_pd",E,r_Xb,N,[[[r_Qb]],[r_Qb]]],[5,"_mm256_round_ps",E,r_Yb,N,[[[r_Rb],[r_la]],[r_Rb]]],[5,"_mm256_ceil_ps",E,r_Yb,N,[[[r_Rb]],[r_Rb]]],[5,"_mm256_floor_ps",E,r_Yb,N,[[[r_Rb]],[r_Rb]]],[5,"_mm256_sqrt_ps",E,r_Zb,N,[[[r_Rb]],[r_Rb]]],[5,"_mm256_sqrt_pd",E,"Return the square root of packed double-precision (64-bit)…",N,[[[r_Qb]],[r_Qb]]],[5,"_mm256_blend_pd",E,r_yb,N,[[[r_Qb],[r_Qb],[r_la]],[r_Qb]]],[5,"_mm256_blend_ps",E,r_zb,N,[[[r_Rb],[r_Rb],[r_la]],[r_Rb]]],[5,"_mm256_blendv_pd",E,r_yb,N,[[[r_Qb],[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_blendv_ps",E,r_zb,N,[[[r_Rb],[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_dp_ps",E,"Conditionally multiply the packed single-precision…",N,[[[r_Rb],[r_Rb],[r_la]],[r_Rb]]],[5,"_mm256_hadd_pd",E,r_0b,N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_hadd_ps",E,r_0b,N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_hsub_pd",E,r_1b,N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_hsub_ps",E,r_1b,N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_xor_pd",E,"Compute the bitwise XOR of packed double-precision…",N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_xor_ps",E,"Compute the bitwise XOR of packed single-precision…",N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm_cmp_pd",E,r_Sb,N,[[[r_Wa],[r_Wa],[r_la]],[r_Wa]]],[5,"_mm256_cmp_pd",E,r_Sb,N,[[[r_Qb],[r_Qb],[r_la]],[r_Qb]]],[5,"_mm_cmp_ps",E,r_ha,N,[[[r_fa],[r_fa],[r_la]],[r_fa]]],[5,"_mm256_cmp_ps",E,r_ha,N,[[[r_Rb],[r_Rb],[r_la]],[r_Rb]]],[5,"_mm_cmp_sd",E,"Compare the lower double-precision (64-bit) floating-point…",N,[[[r_Wa],[r_Wa],[r_la]],[r_Wa]]],[5,"_mm_cmp_ss",E,"Compare the lower single-precision (32-bit) floating-point…",N,[[[r_fa],[r_fa],[r_la]],[r_fa]]],[5,"_mm256_cvtepi32_pd",E,r_2b,N,[[[r_Ka]],[r_Qb]]],[5,"_mm256_cvtepi32_ps",E,r_2b,N,[[[r_3b]],[r_Rb]]],[5,"_mm256_cvtpd_ps",E,r_bb,N,[[[r_Qb]],[r_fa]]],[5,"_mm256_cvtps_epi32",E,r_Ia,N,[[[r_Rb]],[r_3b]]],[5,"_mm256_cvtps_pd",E,r_Ia,N,[[[r_fa]],[r_Qb]]],[5,"_mm256_cvttpd_epi32",E,r_bb,N,[[[r_Qb]],[r_Ka]]],[5,"_mm256_cvtpd_epi32",E,r_bb,N,[[[r_Qb]],[r_Ka]]],[5,"_mm256_cvttps_epi32",E,r_Ia,N,[[[r_Rb]],[r_3b]]],[5,"_mm256_extractf128_ps",E,"Extract 128 bits (composed of 4 packed single-precision…",N,[[[r_Rb],[r_la]],[r_fa]]],[5,"_mm256_extractf128_pd",E,"Extract 128 bits (composed of 2 packed double-precision…",N,[[[r_Qb],[r_la]],[r_Wa]]],[5,"_mm256_extractf128_si256",E,"Extract 128 bits (composed of integer data) from `a`,…",N,[[[r_3b],[r_la]],[r_Ka]]],[5,"_mm256_zeroall",E,"Zero the contents of all XMM or YMM registers.",N,[[]]],[5,"_mm256_zeroupper",E,"Zero the upper 128 bits of all YMM registers; the lower…",N,[[]]],[5,"_mm256_permutevar_ps",E,r_4b,N,[[[r_Rb],[r_3b]],[r_Rb]]],[5,"_mm_permutevar_ps",E,r_4b,N,[[[r_fa],[r_Ka]],[r_fa]]],[5,"_mm256_permute_ps",E,r_4b,N,[[[r_Rb],[r_la]],[r_Rb]]],[5,"_mm_permute_ps",E,r_4b,N,[[[r_fa],[r_la]],[r_fa]]],[5,"_mm256_permutevar_pd",E,r_5b,N,[[[r_Qb],[r_3b]],[r_Qb]]],[5,"_mm_permutevar_pd",E,r_5b,N,[[[r_Wa],[r_Ka]],[r_Wa]]],[5,"_mm256_permute_pd",E,r_5b,N,[[[r_Qb],[r_la]],[r_Qb]]],[5,"_mm_permute_pd",E,r_5b,N,[[[r_Wa],[r_la]],[r_Wa]]],[5,"_mm256_permute2f128_ps",E,"Shuffle 256-bits (composed of 8 packed single-precision…",N,[[[r_Rb],[r_Rb],[r_la]],[r_Rb]]],[5,"_mm256_permute2f128_pd",E,"Shuffle 256-bits (composed of 4 packed double-precision…",N,[[[r_Qb],[r_Qb],[r_la]],[r_Qb]]],[5,"_mm256_permute2f128_si256",E,"Shuffle 258-bits (composed of integer data) selected by…",N,[[[r_3b],[r_3b],[r_la]],[r_3b]]],[5,"_mm256_broadcast_ss",E,r_6b,N,[[[r_na]],[r_Rb]]],[5,"_mm_broadcast_ss",E,r_6b,N,[[[r_na]],[r_fa]]],[5,"_mm256_broadcast_sd",E,"Broadcast a double-precision (64-bit) floating-point…",N,[[[r_eb]],[r_Qb]]],[5,"_mm256_broadcast_ps",E,"Broadcast 128 bits from memory (composed of 4 packed…",N,[[[r_fa]],[r_Rb]]],[5,"_mm256_broadcast_pd",E,"Broadcast 128 bits from memory (composed of 2 packed…",N,[[[r_Wa]],[r_Qb]]],[5,"_mm256_insertf128_ps",E,"Copy `a` to result, then insert 128 bits (composed of 4…",N,[[[r_Rb],[r_fa],[r_la]],[r_Rb]]],[5,"_mm256_insertf128_pd",E,"Copy `a` to result, then insert 128 bits (composed of 2…",N,[[[r_Qb],[r_Wa],[r_la]],[r_Qb]]],[5,"_mm256_insertf128_si256",E,"Copy `a` to result, then insert 128 bits from `b` into…",N,[[[r_3b],[r_Ka],[r_la]],[r_3b]]],[5,"_mm256_insert_epi8",E,"Copy `a` to result, and insert the 8-bit integer `i` into…",N,[[[r_3b],["i8"],[r_la]],[r_3b]]],[5,"_mm256_insert_epi16",E,"Copy `a` to result, and insert the 16-bit integer `i` into…",N,[[[r_3b],[r_Xa],[r_la]],[r_3b]]],[5,"_mm256_insert_epi32",E,"Copy `a` to result, and insert the 32-bit integer `i` into…",N,[[[r_3b],[r_la],[r_la]],[r_3b]]],[5,"_mm256_load_pd",E,r_7b,N,N],[5,"_mm256_store_pd",E,r_8b,N,N],[5,"_mm256_load_ps",E,r_9b,N,N],[5,"_mm256_store_ps",E,r_ac,N,N],[5,"_mm256_loadu_pd",E,r_7b,N,N],[5,"_mm256_storeu_pd",E,r_8b,N,N],[5,"_mm256_loadu_ps",E,r_9b,N,N],[5,"_mm256_storeu_ps",E,r_ac,N,N],[5,"_mm256_load_si256",E,r_bc,N,N],[5,"_mm256_store_si256",E,r_cc,N,N],[5,"_mm256_loadu_si256",E,r_bc,N,N],[5,"_mm256_storeu_si256",E,r_cc,N,N],[5,"_mm256_maskload_pd",E,r_dc,N,N],[5,"_mm256_maskstore_pd",E,r_ec,N,N],[5,"_mm_maskload_pd",E,r_dc,N,N],[5,"_mm_maskstore_pd",E,r_ec,N,N],[5,"_mm256_maskload_ps",E,r_fc,N,N],[5,"_mm256_maskstore_ps",E,r_gc,N,N],[5,"_mm_maskload_ps",E,r_fc,N,N],[5,"_mm_maskstore_ps",E,r_gc,N,N],[5,"_mm256_movehdup_ps",E,r_hc,N,[[[r_Rb]],[r_Rb]]],[5,"_mm256_moveldup_ps",E,r_ic,N,[[[r_Rb]],[r_Rb]]],[5,"_mm256_movedup_pd",E,"Duplicate even-indexed double-precision (64-bit)…",N,[[[r_Qb]],[r_Qb]]],[5,"_mm256_lddqu_si256",E,"Load 256-bits of integer data from unaligned memory into…",N,N],[5,"_mm256_stream_si256",E,"Moves integer data from a 256-bit integer vector to a…",N,N],[5,"_mm256_stream_pd",E,"Moves double-precision values from a 256-bit vector of `[4…",N,N],[5,"_mm256_stream_ps",E,"Moves single-precision floating point values from a…",N,N],[5,"_mm256_rcp_ps",E,"Compute the approximate reciprocal of packed…",N,[[[r_Rb]],[r_Rb]]],[5,"_mm256_rsqrt_ps",E,"Compute the approximate reciprocal square root of packed…",N,[[[r_Rb]],[r_Rb]]],[5,"_mm256_unpackhi_pd",E,r_jc,N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_unpackhi_ps",E,r_qa,N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_unpacklo_pd",E,r_jc,N,[[[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm256_unpacklo_ps",E,r_qa,N,[[[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm256_testz_si256",E,r_kc,N,[[[r_3b],[r_3b]],[r_la]]],[5,"_mm256_testc_si256",E,r_kc,N,[[[r_3b],[r_3b]],[r_la]]],[5,"_mm256_testnzc_si256",E,r_kc,N,[[[r_3b],[r_3b]],[r_la]]],[5,"_mm256_testz_pd",E,r_lc,N,[[[r_Qb],[r_Qb]],[r_la]]],[5,"_mm256_testc_pd",E,r_lc,N,[[[r_Qb],[r_Qb]],[r_la]]],[5,"_mm256_testnzc_pd",E,r_lc,N,[[[r_Qb],[r_Qb]],[r_la]]],[5,"_mm_testz_pd",E,r_mc,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_testc_pd",E,r_mc,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm_testnzc_pd",E,r_mc,N,[[[r_Wa],[r_Wa]],[r_la]]],[5,"_mm256_testz_ps",E,r_lc,N,[[[r_Rb],[r_Rb]],[r_la]]],[5,"_mm256_testc_ps",E,r_lc,N,[[[r_Rb],[r_Rb]],[r_la]]],[5,"_mm256_testnzc_ps",E,r_lc,N,[[[r_Rb],[r_Rb]],[r_la]]],[5,"_mm_testz_ps",E,r_mc,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_testc_ps",E,r_mc,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm_testnzc_ps",E,r_mc,N,[[[r_fa],[r_fa]],[r_la]]],[5,"_mm256_movemask_pd",E,r_nc,N,[[[r_Qb]],[r_la]]],[5,"_mm256_movemask_ps",E,r_nc,N,[[[r_Rb]],[r_la]]],[5,"_mm256_setzero_pd",E,"Return vector of type __m256d with all elements set to zero.",N,[[],[r_Qb]]],[5,"_mm256_setzero_ps",E,"Return vector of type __m256 with all elements set to zero.",N,[[],[r_Rb]]],[5,"_mm256_setzero_si256",E,"Return vector of type __m256i with all elements set to zero.",N,[[],[r_3b]]],[5,"_mm256_set_pd",E,r_fb,N,[[[r_eb],[r_eb],[r_eb],[r_eb]],[r_Qb]]],[5,"_mm256_set_ps",E,r_pc,N,[[[r_na],[r_na],[r_na],[r_na],[r_na],[r_na],[r_na],[r_na]],[r_Rb]]],[5,"_mm256_set_epi8",E,r_qc,N,[[["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"]],[r_3b]]],[5,"_mm256_set_epi16",E,r_rc,N,[[[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa]],[r_3b]]],[5,"_mm256_set_epi32",E,r_sc,N,[[[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la]],[r_3b]]],[5,"_mm256_set_epi64x",E,r_tc,N,[[[r_oc],[r_oc],[r_oc],[r_oc]],[r_3b]]],[5,"_mm256_setr_pd",E,r_fb,N,[[[r_eb],[r_eb],[r_eb],[r_eb]],[r_Qb]]],[5,"_mm256_setr_ps",E,r_pc,N,[[[r_na],[r_na],[r_na],[r_na],[r_na],[r_na],[r_na],[r_na]],[r_Rb]]],[5,"_mm256_setr_epi8",E,r_qc,N,[[["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"]],[r_3b]]],[5,"_mm256_setr_epi16",E,r_rc,N,[[[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa],[r_Xa]],[r_3b]]],[5,"_mm256_setr_epi32",E,r_sc,N,[[[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la]],[r_3b]]],[5,"_mm256_setr_epi64x",E,r_tc,N,[[[r_oc],[r_oc],[r_oc],[r_oc]],[r_3b]]],[5,"_mm256_set1_pd",E,"Broadcast double-precision (64-bit) floating-point value…",N,[[[r_eb]],[r_Qb]]],[5,"_mm256_set1_ps",E,"Broadcast single-precision (32-bit) floating-point value…",N,[[[r_na]],[r_Rb]]],[5,"_mm256_set1_epi8",E,"Broadcast 8-bit integer `a` to all elements of returned…",N,[[["i8"]],[r_3b]]],[5,"_mm256_set1_epi16",E,"Broadcast 16-bit integer `a` to all all elements of…",N,[[[r_Xa]],[r_3b]]],[5,"_mm256_set1_epi32",E,"Broadcast 32-bit integer `a` to all elements of returned…",N,[[[r_la]],[r_3b]]],[5,"_mm256_set1_epi64x",E,"Broadcast 64-bit integer `a` to all elements of returned…",N,[[[r_oc]],[r_3b]]],[5,"_mm256_castpd_ps",E,"Cast vector of type __m256d to type __m256.",N,[[[r_Qb]],[r_Rb]]],[5,"_mm256_castps_pd",E,"Cast vector of type __m256 to type __m256d.",N,[[[r_Rb]],[r_Qb]]],[5,"_mm256_castps_si256",E,"Casts vector of type __m256 to type __m256i.",N,[[[r_Rb]],[r_3b]]],[5,"_mm256_castsi256_ps",E,"Casts vector of type __m256i to type __m256.",N,[[[r_3b]],[r_Rb]]],[5,"_mm256_castpd_si256",E,"Casts vector of type __m256d to type __m256i.",N,[[[r_Qb]],[r_3b]]],[5,"_mm256_castsi256_pd",E,"Casts vector of type __m256i to type __m256d.",N,[[[r_3b]],[r_Qb]]],[5,"_mm256_castps256_ps128",E,"Casts vector of type __m256 to type __m128.",N,[[[r_Rb]],[r_fa]]],[5,"_mm256_castpd256_pd128",E,"Casts vector of type __m256d to type __m128d.",N,[[[r_Qb]],[r_Wa]]],[5,"_mm256_castsi256_si128",E,"Casts vector of type __m256i to type __m128i.",N,[[[r_3b]],[r_Ka]]],[5,"_mm256_castps128_ps256",E,"Casts vector of type __m128 to type __m256; the upper 128…",N,[[[r_fa]],[r_Rb]]],[5,"_mm256_castpd128_pd256",E,"Casts vector of type __m128d to type __m256d; the upper…",N,[[[r_Wa]],[r_Qb]]],[5,"_mm256_castsi128_si256",E,"Casts vector of type __m128i to type __m256i; the upper…",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_zextps128_ps256",E,"Constructs a 256-bit floating-point vector of `[8 x…",N,[[[r_fa]],[r_Rb]]],[5,"_mm256_zextsi128_si256",E,"Constructs a 256-bit integer vector from a 128-bit integer…",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_zextpd128_pd256",E,"Constructs a 256-bit floating-point vector of `[4 x…",N,[[[r_Wa]],[r_Qb]]],[5,"_mm256_undefined_ps",E,"Return vector of type `__m256` with undefined elements.",N,[[],[r_Rb]]],[5,"_mm256_undefined_pd",E,"Return vector of type `__m256d` with undefined elements.",N,[[],[r_Qb]]],[5,"_mm256_undefined_si256",E,"Return vector of type __m256i with undefined elements.",N,[[],[r_3b]]],[5,"_mm256_set_m128",E,r_uc,N,[[[r_fa],[r_fa]],[r_Rb]]],[5,"_mm256_set_m128d",E,r_vc,N,[[[r_Wa],[r_Wa]],[r_Qb]]],[5,"_mm256_set_m128i",E,r_wc,N,[[[r_Ka],[r_Ka]],[r_3b]]],[5,"_mm256_setr_m128",E,r_uc,N,[[[r_fa],[r_fa]],[r_Rb]]],[5,"_mm256_setr_m128d",E,r_vc,N,[[[r_Wa],[r_Wa]],[r_Qb]]],[5,"_mm256_setr_m128i",E,r_wc,N,[[[r_Ka],[r_Ka]],[r_3b]]],[5,"_mm256_loadu2_m128",E,"Load two 128-bit values (composed of 4 packed…",N,N],[5,"_mm256_loadu2_m128d",E,"Load two 128-bit values (composed of 2 packed…",N,N],[5,"_mm256_loadu2_m128i",E,"Load two 128-bit values (composed of integer data) from…",N,N],[5,"_mm256_storeu2_m128",E,"Store the high and low 128-bit halves (each composed of 4…",N,N],[5,"_mm256_storeu2_m128d",E,"Store the high and low 128-bit halves (each composed of 2…",N,N],[5,"_mm256_storeu2_m128i",E,"Store the high and low 128-bit halves (each composed of…",N,N],[5,"_mm256_cvtss_f32",E,r_wd,N,[[[r_Rb]],[r_na]]],[5,"_mm256_abs_epi32",E,r_Qd,N,[[[r_3b]],[r_3b]]],[5,"_mm256_abs_epi16",E,"Computes the absolute values of packed 16-bit integers in…",N,[[[r_3b]],[r_3b]]],[5,"_mm256_abs_epi8",E,"Computes the absolute values of packed 8-bit integers in…",N,[[[r_3b]],[r_3b]]],[5,"_mm256_add_epi64",E,"Add packed 64-bit integers in `a` and `b`.",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_add_epi32",E,r_xc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_add_epi16",E,r_yc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_add_epi8",E,r_zc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_adds_epi8",E,r_Ac,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_adds_epi16",E,r_Bc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_adds_epu8",E,r_Cc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_adds_epu16",E,r_Dc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_alignr_epi8",E,"Concatenate pairs of 16-byte blocks in `a` and `b` into a…",N,[[[r_3b],[r_3b],[r_la]],[r_3b]]],[5,"_mm256_and_si256",E,r_kc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_andnot_si256",E,"Compute the bitwise NOT of 256 bits (representing integer…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_avg_epu16",E,r_Ec,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_avg_epu8",E,r_Fc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm_blend_epi32",E,r_Gc,N,[[[r_Ka],[r_Ka],[r_la]],[r_Ka]]],[5,"_mm256_blend_epi32",E,r_Gc,N,[[[r_3b],[r_3b],[r_la]],[r_3b]]],[5,"_mm256_blend_epi16",E,"Blend packed 16-bit integers from `a` and `b` using…",N,[[[r_3b],[r_3b],[r_la]],[r_3b]]],[5,"_mm256_blendv_epi8",E,"Blend packed 8-bit integers from `a` and `b` using `mask`.",N,[[[r_3b],[r_3b],[r_3b]],[r_3b]]],[5,"_mm_broadcastb_epi8",E,r_Hc,N,[[[r_Ka]],[r_Ka]]],[5,"_mm256_broadcastb_epi8",E,r_Hc,N,[[[r_Ka]],[r_3b]]],[5,"_mm_broadcastd_epi32",E,r_Ic,N,[[[r_Ka]],[r_Ka]]],[5,"_mm256_broadcastd_epi32",E,r_Ic,N,[[[r_Ka]],[r_3b]]],[5,"_mm_broadcastq_epi64",E,r_Jc,N,[[[r_Ka]],[r_Ka]]],[5,"_mm256_broadcastq_epi64",E,r_Jc,N,[[[r_Ka]],[r_3b]]],[5,"_mm_broadcastsd_pd",E,r_Kc,N,[[[r_Wa]],[r_Wa]]],[5,"_mm256_broadcastsd_pd",E,r_Kc,N,[[[r_Wa]],[r_Qb]]],[5,"_mm256_broadcastsi128_si256",E,"Broadcast 128 bits of integer data from a to all 128-bit…",N,[[[r_Ka]],[r_3b]]],[5,"_mm_broadcastss_ps",E,r_Lc,N,[[[r_fa]],[r_fa]]],[5,"_mm256_broadcastss_ps",E,r_Lc,N,[[[r_fa]],[r_Rb]]],[5,"_mm_broadcastw_epi16",E,r_Mc,N,[[[r_Ka]],[r_Ka]]],[5,"_mm256_broadcastw_epi16",E,r_Mc,N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cmpeq_epi64",E,"Compare packed 64-bit integers in `a` and `b` for equality.",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_cmpeq_epi32",E,r_Nc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_cmpeq_epi16",E,r_Oc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_cmpeq_epi8",E,r_Pc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_cmpgt_epi64",E,r_Qc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_cmpgt_epi32",E,r_Rc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_cmpgt_epi16",E,r_Sc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_cmpgt_epi8",E,r_Tc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_cvtepi16_epi32",E,"Sign-extend 16-bit integers to 32-bit integers.",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cvtepi16_epi64",E,"Sign-extend 16-bit integers to 64-bit integers.",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cvtepi32_epi64",E,"Sign-extend 32-bit integers to 64-bit integers.",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cvtepi8_epi16",E,"Sign-extend 8-bit integers to 16-bit integers.",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cvtepi8_epi32",E,"Sign-extend 8-bit integers to 32-bit integers.",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cvtepi8_epi64",E,"Sign-extend 8-bit integers to 64-bit integers.",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cvtepu16_epi32",E,r_Gb,N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cvtepu16_epi64",E,"Zero-extend the lower four unsigned 16-bit integers in `a`…",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cvtepu32_epi64",E,"Zero-extend unsigned 32-bit integers in `a` to 64-bit…",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cvtepu8_epi16",E,"Zero-extend unsigned 8-bit integers in `a` to 16-bit…",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cvtepu8_epi32",E,"Zero-extend the lower eight unsigned 8-bit integers in `a`…",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_cvtepu8_epi64",E,"Zero-extend the lower four unsigned 8-bit integers in `a`…",N,[[[r_Ka]],[r_3b]]],[5,"_mm256_extracti128_si256",E,"Extract 128 bits (of integer data) from `a` selected with…",N,[[[r_3b],[r_la]],[r_Ka]]],[5,"_mm256_hadd_epi16",E,r_Uc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_hadd_epi32",E,"Horizontally add adjacent pairs of 32-bit integers in `a`…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_hadds_epi16",E,r_Uc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_hsub_epi16",E,r_Vc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_hsub_epi32",E,"Horizontally subtract adjacent pairs of 32-bit integers in…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_hsubs_epi16",E,r_Vc,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm_i32gather_epi32",E,r_Wc,N,N],[5,"_mm_mask_i32gather_epi32",E,r_Wc,N,N],[5,"_mm256_i32gather_epi32",E,r_Wc,N,N],[5,"_mm256_mask_i32gather_epi32",E,r_Wc,N,N],[5,"_mm_i32gather_ps",E,r_Wc,N,N],[5,"_mm_mask_i32gather_ps",E,r_Wc,N,N],[5,"_mm256_i32gather_ps",E,r_Wc,N,N],[5,"_mm256_mask_i32gather_ps",E,r_Wc,N,N],[5,"_mm_i32gather_epi64",E,r_Wc,N,N],[5,"_mm_mask_i32gather_epi64",E,r_Wc,N,N],[5,"_mm256_i32gather_epi64",E,r_Wc,N,N],[5,"_mm256_mask_i32gather_epi64",E,r_Wc,N,N],[5,"_mm_i32gather_pd",E,r_Wc,N,N],[5,"_mm_mask_i32gather_pd",E,r_Wc,N,N],[5,"_mm256_i32gather_pd",E,r_Wc,N,N],[5,"_mm256_mask_i32gather_pd",E,r_Wc,N,N],[5,"_mm_i64gather_epi32",E,r_Wc,N,N],[5,"_mm_mask_i64gather_epi32",E,r_Wc,N,N],[5,"_mm256_i64gather_epi32",E,r_Wc,N,N],[5,"_mm256_mask_i64gather_epi32",E,r_Wc,N,N],[5,"_mm_i64gather_ps",E,r_Wc,N,N],[5,"_mm_mask_i64gather_ps",E,r_Wc,N,N],[5,"_mm256_i64gather_ps",E,r_Wc,N,N],[5,"_mm256_mask_i64gather_ps",E,r_Wc,N,N],[5,"_mm_i64gather_epi64",E,r_Wc,N,N],[5,"_mm_mask_i64gather_epi64",E,r_Wc,N,N],[5,"_mm256_i64gather_epi64",E,r_Wc,N,N],[5,"_mm256_mask_i64gather_epi64",E,r_Wc,N,N],[5,"_mm_i64gather_pd",E,r_Wc,N,N],[5,"_mm_mask_i64gather_pd",E,r_Wc,N,N],[5,"_mm256_i64gather_pd",E,r_Wc,N,N],[5,"_mm256_mask_i64gather_pd",E,r_Wc,N,N],[5,"_mm256_inserti128_si256",E,"Copy `a` to `dst`, then insert 128 bits (of integer data)…",N,[[[r_3b],[r_Ka],[r_la]],[r_3b]]],[5,"_mm256_madd_epi16",E,"Multiply packed signed 16-bit integers in `a` and `b`,…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_maddubs_epi16",E,"Vertically multiply each unsigned 8-bit integer from `a`…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm_maskload_epi32",E,r_Xc,N,N],[5,"_mm256_maskload_epi32",E,r_Xc,N,N],[5,"_mm_maskload_epi64",E,r_Yc,N,N],[5,"_mm256_maskload_epi64",E,r_Yc,N,N],[5,"_mm_maskstore_epi32",E,r_Zc,N,N],[5,"_mm256_maskstore_epi32",E,r_Zc,N,N],[5,"_mm_maskstore_epi64",E,r_0c,N,N],[5,"_mm256_maskstore_epi64",E,r_0c,N,N],[5,"_mm256_max_epi16",E,r_La,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_max_epi32",E,r_Cb,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_max_epi8",E,r_1c,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_max_epu16",E,r_Bb,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_max_epu32",E,r_Db,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_max_epu8",E,r_Ma,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_min_epi16",E,r_La,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_min_epi32",E,r_Cb,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_min_epi8",E,r_1c,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_min_epu16",E,r_Bb,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_min_epu32",E,r_Db,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_min_epu8",E,r_Ma,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_movemask_epi8",E,"Create mask from the most significant bit of each 8-bit…",N,[[[r_3b]],[r_la]]],[5,"_mm256_mpsadbw_epu8",E,"Compute the sum of absolute differences (SADs) of…",N,[[[r_3b],[r_3b],[r_la]],[r_3b]]],[5,"_mm256_mul_epi32",E,r_2c,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_mul_epu32",E,r_3c,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_mulhi_epi16",E,r_4c,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_mulhi_epu16",E,"Multiply the packed unsigned 16-bit integers in `a` and…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_mullo_epi16",E,r_4c,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_mullo_epi32",E,r_5c,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_mulhrs_epi16",E,"Multiply packed 16-bit integers in `a` and `b`, producing…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_or_si256",E,"Compute the bitwise OR of 256 bits (representing integer…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_packs_epi16",E,r_0a,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_packs_epi32",E,r_Eb,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_packus_epi16",E,r_0a,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_packus_epi32",E,r_Eb,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_permutevar8x32_epi32",E,"Permutes packed 32-bit integers from `a` according to the…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_permute4x64_epi64",E,"Permutes 64-bit integers from `a` using control mask `imm8`.",N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_permute2x128_si256",E,"Shuffle 128-bits of integer data selected by `imm8` from…",N,[[[r_3b],[r_3b],[r_la]],[r_3b]]],[5,"_mm256_permute4x64_pd",E,"Shuffle 64-bit floating-point elements in `a` across lanes…",N,[[[r_Qb],[r_la]],[r_Qb]]],[5,"_mm256_permutevar8x32_ps",E,"Shuffle eight 32-bit foating-point elements in `a` across…",N,[[[r_Rb],[r_3b]],[r_Rb]]],[5,"_mm256_sad_epu8",E,"Compute the absolute differences of packed unsigned 8-bit…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_shuffle_epi8",E,r_6c,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_shuffle_epi32",E,"Shuffle 32-bit integers in 128-bit lanes of `a` using the…",N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_shufflehi_epi16",E,"Shuffle 16-bit integers in the high 64 bits of 128-bit…",N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_shufflelo_epi16",E,"Shuffle 16-bit integers in the low 64 bits of 128-bit…",N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_sign_epi16",E,r_wb,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_sign_epi32",E,r_xb,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_sign_epi8",E,r_vb,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_sll_epi16",E,r_7c,N,[[[r_3b],[r_Ka]],[r_3b]]],[5,"_mm256_sll_epi32",E,r_8c,N,[[[r_3b],[r_Ka]],[r_3b]]],[5,"_mm256_sll_epi64",E,r_9c,N,[[[r_3b],[r_Ka]],[r_3b]]],[5,"_mm256_slli_epi16",E,r_ad,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_slli_epi32",E,r_bd,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_slli_epi64",E,r_cd,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_slli_si256",E,r_dd,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_bslli_epi128",E,r_dd,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm_sllv_epi32",E,r_ed,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm256_sllv_epi32",E,r_ed,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm_sllv_epi64",E,r_fd,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm256_sllv_epi64",E,r_fd,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_sra_epi16",E,r_Ta,N,[[[r_3b],[r_Ka]],[r_3b]]],[5,"_mm256_sra_epi32",E,r_Va,N,[[[r_3b],[r_Ka]],[r_3b]]],[5,"_mm256_srai_epi16",E,r_Sa,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_srai_epi32",E,r_Ua,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm_srav_epi32",E,r_gd,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm256_srav_epi32",E,r_gd,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_srli_si256",E,r_hd,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_bsrli_epi128",E,r_hd,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_srl_epi16",E,r_Ta,N,[[[r_3b],[r_Ka]],[r_3b]]],[5,"_mm256_srl_epi32",E,r_Va,N,[[[r_3b],[r_Ka]],[r_3b]]],[5,"_mm256_srl_epi64",E,r_id,N,[[[r_3b],[r_Ka]],[r_3b]]],[5,"_mm256_srli_epi16",E,r_Sa,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_srli_epi32",E,r_Ua,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm256_srli_epi64",E,r_jd,N,[[[r_3b],[r_la]],[r_3b]]],[5,"_mm_srlv_epi32",E,r_gd,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm256_srlv_epi32",E,r_gd,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm_srlv_epi64",E,r_kd,N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm256_srlv_epi64",E,r_kd,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_sub_epi16",E,r_Pa,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_sub_epi32",E,"Subtract packed 32-bit integers in `b` from packed 16-bit…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_sub_epi64",E,"Subtract packed 64-bit integers in `b` from packed 16-bit…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_sub_epi8",E,"Subtract packed 8-bit integers in `b` from packed 16-bit…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_subs_epi16",E,r_Pa,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_subs_epi8",E,r_Oa,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_subs_epu16",E,r_ld,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_subs_epu8",E,r_md,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_unpackhi_epi8",E,r_nd,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_unpacklo_epi8",E,r_od,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_unpackhi_epi16",E,r_pd,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_unpacklo_epi16",E,r_qd,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_unpackhi_epi32",E,r_rd,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_unpacklo_epi32",E,r_sd,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_unpackhi_epi64",E,r_td,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_unpacklo_epi64",E,r_ud,N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_xor_si256",E,"Compute the bitwise XOR of 256 bits (representing integer…",N,[[[r_3b],[r_3b]],[r_3b]]],[5,"_mm256_extract_epi8",E,r_vd,N,[[[r_3b],[r_la]],["i8"]]],[5,"_mm256_extract_epi16",E,"Extract a 16-bit integer from `a`, selected with `imm8`.…",N,[[[r_3b],[r_la]],[r_Xa]]],[5,"_mm256_extract_epi32",E,"Extract a 32-bit integer from `a`, selected with `imm8`.",N,[[[r_3b],[r_la]],[r_la]]],[5,"_mm256_cvtsd_f64",E,"Returns the first element of the input vector of `[4 x…",N,[[[r_Qb]],[r_eb]]],[5,"_mm256_cvtsi256_si32",E,r_wd,N,[[[r_3b]],[r_la]]],[5,"_mm_fmadd_pd",E,r_xd,N,[[[r_Wa],[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm256_fmadd_pd",E,r_xd,N,[[[r_Qb],[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm_fmadd_ps",E,r_yd,N,[[[r_fa],[r_fa],[r_fa]],[r_fa]]],[5,"_mm256_fmadd_ps",E,r_yd,N,[[[r_Rb],[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm_fmadd_sd",E,r_zd,N,[[[r_Wa],[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_fmadd_ss",E,r_Ad,N,[[[r_fa],[r_fa],[r_fa]],[r_fa]]],[5,"_mm_fmaddsub_pd",E,r_xd,N,[[[r_Wa],[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm256_fmaddsub_pd",E,r_xd,N,[[[r_Qb],[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm_fmaddsub_ps",E,r_yd,N,[[[r_fa],[r_fa],[r_fa]],[r_fa]]],[5,"_mm256_fmaddsub_ps",E,r_yd,N,[[[r_Rb],[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm_fmsub_pd",E,r_xd,N,[[[r_Wa],[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm256_fmsub_pd",E,r_xd,N,[[[r_Qb],[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm_fmsub_ps",E,r_yd,N,[[[r_fa],[r_fa],[r_fa]],[r_fa]]],[5,"_mm256_fmsub_ps",E,r_yd,N,[[[r_Rb],[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm_fmsub_sd",E,r_zd,N,[[[r_Wa],[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_fmsub_ss",E,r_Ad,N,[[[r_fa],[r_fa],[r_fa]],[r_fa]]],[5,"_mm_fmsubadd_pd",E,r_xd,N,[[[r_Wa],[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm256_fmsubadd_pd",E,r_xd,N,[[[r_Qb],[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm_fmsubadd_ps",E,r_yd,N,[[[r_fa],[r_fa],[r_fa]],[r_fa]]],[5,"_mm256_fmsubadd_ps",E,r_yd,N,[[[r_Rb],[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm_fnmadd_pd",E,r_xd,N,[[[r_Wa],[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm256_fnmadd_pd",E,r_xd,N,[[[r_Qb],[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm_fnmadd_ps",E,r_yd,N,[[[r_fa],[r_fa],[r_fa]],[r_fa]]],[5,"_mm256_fnmadd_ps",E,r_yd,N,[[[r_Rb],[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm_fnmadd_sd",E,r_zd,N,[[[r_Wa],[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_fnmadd_ss",E,r_Ad,N,[[[r_fa],[r_fa],[r_fa]],[r_fa]]],[5,"_mm_fnmsub_pd",E,r_xd,N,[[[r_Wa],[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm256_fnmsub_pd",E,r_xd,N,[[[r_Qb],[r_Qb],[r_Qb]],[r_Qb]]],[5,"_mm_fnmsub_ps",E,r_yd,N,[[[r_fa],[r_fa],[r_fa]],[r_fa]]],[5,"_mm256_fnmsub_ps",E,r_yd,N,[[[r_Rb],[r_Rb],[r_Rb]],[r_Rb]]],[5,"_mm_fnmsub_sd",E,r_zd,N,[[[r_Wa],[r_Wa],[r_Wa]],[r_Wa]]],[5,"_mm_fnmsub_ss",E,r_Ad,N,[[[r_fa],[r_fa],[r_fa]],[r_fa]]],[5,"_lzcnt_u32",E,r_Zd,N,[[[r_pa]],[r_pa]]],[5,"_popcnt32",E,r_0d,N,[[[r_la]],[r_la]]],[5,"_bextr_u32",E,r_1d,N,[[[r_pa],[r_pa],[r_pa]],[r_pa]]],[5,"_bextr2_u32",E,r_2d,N,[[[r_pa],[r_pa]],[r_pa]]],[5,"_andn_u32",E,r_3d,N,[[[r_pa],[r_pa]],[r_pa]]],[5,"_blsi_u32",E,r_4d,N,[[[r_pa]],[r_pa]]],[5,"_blsmsk_u32",E,r_5d,N,[[[r_pa]],[r_pa]]],[5,"_blsr_u32",E,r_6d,N,[[[r_pa]],[r_pa]]],[5,"_tzcnt_u32",E,r_Bd,N,[[[r_pa]],[r_pa]]],[5,"_mm_tzcnt_32",E,r_Bd,N,[[[r_pa]],[r_la]]],[5,"_mulx_u32",E,r_7d,N,[[[r_pa],[r_pa],[r_pa]],[r_pa]]],[5,"_bzhi_u32",E,r_8d,N,[[[r_pa],[r_pa]],[r_pa]]],[5,"_pdep_u32",E,r_9d,N,[[[r_pa],[r_pa]],[r_pa]]],[5,"_pext_u32",E,r_ae,N,[[[r_pa],[r_pa]],[r_pa]]],[5,"_mm_extract_si64",E,"Extracts the bit range specified by `y` from the lower 64…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_insert_si64",E,"Inserts the `[length:0]` bits of `y` into `x` at `index`.",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_stream_sd",E,r_Cd,N,N],[5,"_mm_stream_ss",E,r_Cd,N,N],[5,"_blcfill_u32",E,r_Dd,N,[[[r_pa]],[r_pa]]],[5,"_blcfill_u64",E,r_Dd,N,[[[r_Fd]],[r_Fd]]],[5,"_blci_u32",E,r_Ed,N,[[[r_pa]],[r_pa]]],[5,"_blci_u64",E,r_Ed,N,[[[r_Fd]],[r_Fd]]],[5,"_blcic_u32",E,r_Gd,N,[[[r_pa]],[r_pa]]],[5,"_blcic_u64",E,r_Gd,N,[[[r_Fd]],[r_Fd]]],[5,"_blcmsk_u32",E,r_Gd,N,[[[r_pa]],[r_pa]]],[5,"_blcmsk_u64",E,r_Gd,N,[[[r_Fd]],[r_Fd]]],[5,"_blcs_u32",E,r_Hd,N,[[[r_pa]],[r_pa]]],[5,"_blcs_u64",E,r_Hd,N,[[[r_Fd]],[r_Fd]]],[5,"_blsfill_u32",E,r_Id,N,[[[r_pa]],[r_pa]]],[5,"_blsfill_u64",E,r_Id,N,[[[r_Fd]],[r_Fd]]],[5,"_blsic_u32",E,r_Jd,N,[[[r_pa]],[r_pa]]],[5,"_blsic_u64",E,r_Jd,N,[[[r_Fd]],[r_Fd]]],[5,"_t1mskc_u32",E,r_Kd,N,[[[r_pa]],[r_pa]]],[5,"_t1mskc_u64",E,r_Kd,N,[[[r_Fd]],[r_Fd]]],[5,"_tzmsk_u32",E,r_Ld,N,[[[r_pa]],[r_pa]]],[5,"_tzmsk_u64",E,r_Ld,N,[[[r_Fd]],[r_Fd]]],[5,"_mm_setzero_si64",E,"Constructs a 64-bit integer vector initialized to zero.",N,[[],[r_sa]]],[5,"_mm_add_pi8",E,r_zc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_paddb",E,r_zc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_add_pi16",E,r_yc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_paddw",E,r_yc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_add_pi32",E,r_xc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_paddd",E,r_xc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_adds_pi8",E,r_Ac,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_paddsb",E,r_Ac,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_adds_pi16",E,r_Bc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_paddsw",E,r_Bc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_adds_pu8",E,r_Cc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_paddusb",E,r_Cc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_adds_pu16",E,r_Dc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_paddusw",E,r_Dc,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_sub_pi8",E,r_Oa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_psubb",E,r_Oa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_sub_pi16",E,r_Pa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_psubw",E,r_Pa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_sub_pi32",E,r_Md,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_psubd",E,r_Md,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_subs_pi8",E,r_Oa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_psubsb",E,r_Oa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_subs_pi16",E,r_Pa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_psubsw",E,r_Pa,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_subs_pu8",E,r_md,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_psubusb",E,r_md,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_subs_pu16",E,r_ld,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_m_psubusw",E,r_ld,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_packs_pi16",E,r_0a,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_packs_pi32",E,r_Eb,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_cmpgt_pi8",E,r_Nd,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_cmpgt_pi16",E,r_Nd,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_cmpgt_pi32",E,r_Nd,N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_unpackhi_pi16",E,"Unpacks the upper two elements from two `i16x4` vectors…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_unpackhi_pi8",E,"Unpacks the upper four elements from two `i8x8` vectors…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_unpacklo_pi8",E,"Unpacks the lower four elements from two `i8x8` vectors…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_unpacklo_pi16",E,"Unpacks the lower two elements from two `i16x4` vectors…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_unpackhi_pi32",E,"Unpacks the upper element from two `i32x2` vectors and…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_unpacklo_pi32",E,"Unpacks the lower element from two `i32x2` vectors and…",N,[[[r_sa],[r_sa]],[r_sa]]],[5,"_mm_set_pi16",E,"Set packed 16-bit integers in dst with the supplied values.",N,[[[r_Xa],[r_Xa],[r_Xa],[r_Xa]],[r_sa]]],[5,"_mm_set_pi32",E,"Set packed 32-bit integers in dst with the supplied values.",N,[[[r_la],[r_la]],[r_sa]]],[5,"_mm_set_pi8",E,"Set packed 8-bit integers in dst with the supplied values.",N,[[["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"]],[r_sa]]],[5,"_mm_set1_pi16",E,"Broadcast 16-bit integer a to all all elements of dst.",N,[[[r_Xa]],[r_sa]]],[5,"_mm_set1_pi32",E,"Broadcast 32-bit integer a to all all elements of dst.",N,[[[r_la]],[r_sa]]],[5,"_mm_set1_pi8",E,"Broadcast 8-bit integer a to all all elements of dst.",N,[[["i8"]],[r_sa]]],[5,"_mm_setr_pi16",E,"Set packed 16-bit integers in dst with the supplied values…",N,[[[r_Xa],[r_Xa],[r_Xa],[r_Xa]],[r_sa]]],[5,"_mm_setr_pi32",E,"Set packed 32-bit integers in dst with the supplied values…",N,[[[r_la],[r_la]],[r_sa]]],[5,"_mm_setr_pi8",E,"Set packed 8-bit integers in dst with the supplied values…",N,[[["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"],["i8"]],[r_sa]]],[5,"_mm_empty",E,r_Od,N,[[]]],[5,"_m_empty",E,r_Od,N,[[]]],[5,"_mm_cvtsi32_si64",E,"Copy 32-bit integer `a` to the lower elements of the…",N,[[[r_la]],[r_sa]]],[5,"_mm_cvtsi64_si32",E,"Return the lower 32-bit integer in `a`.",N,[[[r_sa]],[r_la]]],[5,"_mm_clmulepi64_si128",E,"Perform a carry-less multiplication of two 64-bit…",N,[[[r_Ka],[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_aesdec_si128",E,"Perform one round of an AES decryption flow on data…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_aesdeclast_si128",E,"Perform the last round of an AES decryption flow on data…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_aesenc_si128",E,"Perform one round of an AES encryption flow on data…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_aesenclast_si128",E,"Perform the last round of an AES encryption flow on data…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_aesimc_si128",E,"Perform the `InvMixColumns` transformation on `a`.",N,[[[r_Ka]],[r_Ka]]],[5,"_mm_aeskeygenassist_si128",E,"Assist in expanding the AES cipher key.",N,[[[r_Ka],[r_la]],[r_Ka]]],[5,"_rdrand16_step",E,"Read a hardware generated 16-bit random value and store…",N,[[["u16"]],[r_la]]],[5,"_rdrand32_step",E,"Read a hardware generated 32-bit random value and store…",N,[[[r_pa]],[r_la]]],[5,"_rdseed16_step",E,"Read a 16-bit NIST SP800-90B and SP800-90C compliant…",N,[[["u16"]],[r_la]]],[5,"_rdseed32_step",E,"Read a 32-bit NIST SP800-90B and SP800-90C compliant…",N,[[[r_pa]],[r_la]]],[5,"_mm_sha1msg1_epu32",E,"Perform an intermediate calculation for the next four SHA1…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sha1msg2_epu32",E,"Perform the final calculation for the next four SHA1…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sha1nexte_epu32",E,"Calculate SHA1 state variable E after four rounds of…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sha1rnds4_epu32",E,"Perform four rounds of SHA1 operation using an initial…",N,[[[r_Ka],[r_Ka],[r_la]],[r_Ka]]],[5,"_mm_sha256msg1_epu32",E,"Perform an intermediate calculation for the next four…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sha256msg2_epu32",E,"Perform the final calculation for the next four SHA256…",N,[[[r_Ka],[r_Ka]],[r_Ka]]],[5,"_mm_sha256rnds2_epu32",E,"Perform 2 rounds of SHA256 operation using an initial…",N,[[[r_Ka],[r_Ka],[r_Ka]],[r_Ka]]],[5,"_addcarry_u32",E,r_Pd,N,[[["u8"],[r_pa],[r_pa],[r_pa]],["u8"]]],[5,"_addcarryx_u32",E,r_Pd,N,[[["u8"],[r_pa],[r_pa],[r_pa]],["u8"]]],[5,"_subborrow_u32",E,r_Pd,N,[[["u8"],[r_pa],[r_pa],[r_pa]],["u8"]]],[5,"ud2",E,"Generates the trap instruction `UD2`",N,N],[5,"_mm512_abs_epi32",E,r_Qd,N,[[[r_Rd]],[r_Rd]]],[5,"_mm512_mask_abs_epi32",E,r_Sd,N,[[[r_Rd],[r_Td],[r_Rd]],[r_Rd]]],[5,"_mm512_maskz_abs_epi32",E,r_Sd,N,[[[r_Td],[r_Rd]],[r_Rd]]],[5,"_mm512_setzero_si512",E,"Return vector of type `__m512i` with all elements set to…",N,[[],[r_Rd]]],[5,"_mm512_setr_epi32",E,"Set packed 32-bit integers in `dst` with the supplied…",N,[[[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la],[r_la]],[r_Rd]]],[5,"_fxsave64",E,r_Ud,N,N],[5,"_fxrstor64",E,r_Vd,N,N],[5,"_mm_cvtss_si64",E,r_ma,N,[[[r_fa]],[r_oc]]],[5,"_mm_cvttss_si64",E,r_ma,N,[[[r_fa]],[r_oc]]],[5,"_mm_cvtsi64_ss",E,"Convert a 64 bit integer to a 32 bit float. The result…",N,[[[r_fa],[r_oc]],[r_fa]]],[5,"_mm_cvtsd_si64",E,r_cb,N,[[[r_Wa]],[r_oc]]],[5,"_mm_cvtsd_si64x",E,"Alias for `_mm_cvtsd_si64`",N,[[[r_Wa]],[r_oc]]],[5,"_mm_cvttsd_si64",E,r_cb,N,[[[r_Wa]],[r_oc]]],[5,"_mm_cvttsd_si64x",E,"Alias for `_mm_cvttsd_si64`",N,[[[r_Wa]],[r_oc]]],[5,"_mm_stream_si64",E,"Stores a 64-bit integer value in the specified memory…",N,N],[5,"_mm_cvtsi64_si128",E,r_Wd,N,[[[r_oc]],[r_Ka]]],[5,"_mm_cvtsi64x_si128",E,r_Wd,N,[[[r_oc]],[r_Ka]]],[5,"_mm_cvtsi128_si64",E,r_Xd,N,[[[r_Ka]],[r_oc]]],[5,"_mm_cvtsi128_si64x",E,r_Xd,N,[[[r_Ka]],[r_oc]]],[5,"_mm_cvtsi64_sd",E,r_Yd,N,[[[r_Wa],[r_oc]],[r_Wa]]],[5,"_mm_cvtsi64x_sd",E,r_Yd,N,[[[r_Wa],[r_oc]],[r_Wa]]],[5,"_mm_extract_epi64",E,"Extract an 64-bit integer from `a` selected with `imm8`",N,[[[r_Ka],[r_la]],[r_oc]]],[5,"_mm_insert_epi64",E,"Return a copy of `a` with the 64-bit integer from `i`…",N,[[[r_Ka],[r_oc],[r_la]],[r_Ka]]],[5,"_mm_crc32_u64",E,r_Ob,N,[[[r_Fd],[r_Fd]],[r_Fd]]],[5,"_xsave64",E,r_da,N,N],[5,"_xrstor64",E,r_ea,N,N],[5,"_xsaveopt64",E,r_da,N,N],[5,"_xsavec64",E,r_da,N,N],[5,"_xsaves64",E,r_da,N,N],[5,"_xrstors64",E,r_ea,N,N],[5,"_lzcnt_u64",E,r_Zd,N,[[[r_Fd]],[r_Fd]]],[5,"_popcnt64",E,r_0d,N,[[[r_oc]],[r_la]]],[5,"_mm256_insert_epi64",E,"Copy `a` to result, and insert the 64-bit integer `i` into…",N,[[[r_3b],[r_oc],[r_la]],[r_3b]]],[5,"_bextr_u64",E,r_1d,N,[[[r_Fd],[r_pa],[r_pa]],[r_Fd]]],[5,"_bextr2_u64",E,r_2d,N,[[[r_Fd],[r_Fd]],[r_Fd]]],[5,"_andn_u64",E,r_3d,N,[[[r_Fd],[r_Fd]],[r_Fd]]],[5,"_blsi_u64",E,r_4d,N,[[[r_Fd]],[r_Fd]]],[5,"_blsmsk_u64",E,r_5d,N,[[[r_Fd]],[r_Fd]]],[5,"_blsr_u64",E,r_6d,N,[[[r_Fd]],[r_Fd]]],[5,"_tzcnt_u64",E,r_Bd,N,[[[r_Fd]],[r_Fd]]],[5,"_mm_tzcnt_64",E,r_Bd,N,[[[r_Fd]],[r_oc]]],[5,"_mulx_u64",E,r_7d,N,[[[r_Fd],[r_Fd],[r_Fd]],[r_Fd]]],[5,"_bzhi_u64",E,r_8d,N,[[[r_Fd],[r_pa]],[r_Fd]]],[5,"_pdep_u64",E,r_9d,N,[[[r_Fd],[r_Fd]],[r_Fd]]],[5,"_pext_u64",E,r_ae,N,[[[r_Fd],[r_Fd]],[r_Fd]]],[5,"_mm256_extract_epi64",E,"Extract a 64-bit integer from `a`, selected with `imm8`.",N,[[[r_3b],[r_la]],[r_oc]]],[5,"_bswap64",E,r_be,N,[[[r_oc]],[r_oc]]],[5,"_rdrand64_step",E,"Read a hardware generated 64-bit random value and store…",N,[[[r_Fd]],[r_la]]],[5,"_rdseed64_step",E,"Read a 64-bit NIST SP800-90B and SP800-90C compliant…",N,[[[r_Fd]],[r_la]]],[5,"cmpxchg16b",E,"Compare and exchange 16 bytes (128 bits) of data atomically.",N,N],[5,"_addcarry_u64",E,r_ce,N,[[["u8"],[r_Fd],[r_Fd],[r_Fd]],["u8"]]],[5,"_addcarryx_u64",E,r_ce,N,[[["u8"],[r_Fd],[r_Fd],[r_Fd]],["u8"]]],[5,"_subborrow_u64",E,r_ce,N,[[["u8"],[r_Fd],[r_Fd],[r_Fd]],["u8"]]],[6,r_Td,E,"The `__mmask16` type used in AVX-512 intrinsics, a 16-bit…",N,N],[17,"_XCR_XFEATURE_ENABLED_MASK",E,"`XFEATURE_ENABLED_MASK` for `XCR`",N,N],[17,"_MM_EXCEPT_INVALID",E,r_ra,N,N],[17,"_MM_EXCEPT_DENORM",E,r_ra,N,N],[17,"_MM_EXCEPT_DIV_ZERO",E,r_ra,N,N],[17,"_MM_EXCEPT_OVERFLOW",E,r_ra,N,N],[17,"_MM_EXCEPT_UNDERFLOW",E,r_ra,N,N],[17,"_MM_EXCEPT_INEXACT",E,r_ra,N,N],[17,"_MM_EXCEPT_MASK",E,"See `_MM_GET_EXCEPTION_STATE`",N,N],[17,"_MM_MASK_INVALID",E,r_ra,N,N],[17,"_MM_MASK_DENORM",E,r_ra,N,N],[17,"_MM_MASK_DIV_ZERO",E,r_ra,N,N],[17,"_MM_MASK_OVERFLOW",E,r_ra,N,N],[17,"_MM_MASK_UNDERFLOW",E,r_ra,N,N],[17,"_MM_MASK_INEXACT",E,r_ra,N,N],[17,"_MM_MASK_MASK",E,"See `_MM_GET_EXCEPTION_MASK`",N,N],[17,"_MM_ROUND_NEAREST",E,r_ra,N,N],[17,"_MM_ROUND_DOWN",E,r_ra,N,N],[17,"_MM_ROUND_UP",E,r_ra,N,N],[17,"_MM_ROUND_TOWARD_ZERO",E,r_ra,N,N],[17,"_MM_ROUND_MASK",E,"See `_MM_GET_ROUNDING_MODE`",N,N],[17,"_MM_FLUSH_ZERO_MASK",E,"See `_MM_GET_FLUSH_ZERO_MODE`",N,N],[17,"_MM_FLUSH_ZERO_ON",E,r_ra,N,N],[17,"_MM_FLUSH_ZERO_OFF",E,r_ra,N,N],[17,"_MM_HINT_T0",E,r_de,N,N],[17,"_MM_HINT_T1",E,r_de,N,N],[17,"_MM_HINT_T2",E,r_de,N,N],[17,"_MM_HINT_NTA",E,r_de,N,N],[17,"_MM_FROUND_TO_NEAREST_INT",E,"round to nearest",N,N],[17,"_MM_FROUND_TO_NEG_INF",E,"round down",N,N],[17,"_MM_FROUND_TO_POS_INF",E,"round up",N,N],[17,"_MM_FROUND_TO_ZERO",E,"truncate",N,N],[17,"_MM_FROUND_CUR_DIRECTION",E,"use MXCSR.RC; see `vendor::_MM_SET_ROUNDING_MODE`",N,N],[17,"_MM_FROUND_RAISE_EXC",E,"do not suppress exceptions",N,N],[17,"_MM_FROUND_NO_EXC",E,"suppress exceptions",N,N],[17,"_MM_FROUND_NINT",E,"round to nearest and do not suppress exceptions",N,N],[17,"_MM_FROUND_FLOOR",E,"round down and do not suppress exceptions",N,N],[17,"_MM_FROUND_CEIL",E,"round up and do not suppress exceptions",N,N],[17,"_MM_FROUND_TRUNC",E,"truncate and do not suppress exceptions",N,N],[17,"_MM_FROUND_RINT",E,"use MXCSR.RC and do not suppress exceptions; see…",N,N],[17,"_MM_FROUND_NEARBYINT",E,"use MXCSR.RC and suppress exceptions; see…",N,N],[17,"_SIDD_UBYTE_OPS",E,"String contains unsigned 8-bit characters (Default)",N,N],[17,"_SIDD_UWORD_OPS",E,r_ee,N,N],[17,"_SIDD_SBYTE_OPS",E,"String contains signed 8-bit characters",N,N],[17,"_SIDD_SWORD_OPS",E,r_ee,N,N],[17,"_SIDD_CMP_EQUAL_ANY",E,"For each character in `a`, find if it is in `b` (Default)",N,N],[17,"_SIDD_CMP_RANGES",E,"For each character in `a`, determine if `b[0] <= c <= b[1]…",N,N],[17,"_SIDD_CMP_EQUAL_EACH",E,"The strings defined by `a` and `b` are equal",N,N],[17,"_SIDD_CMP_EQUAL_ORDERED",E,"Search for the defined substring in the target",N,N],[17,"_SIDD_POSITIVE_POLARITY",E,"Do not negate results (Default)",N,N],[17,"_SIDD_NEGATIVE_POLARITY",E,"Negate results",N,N],[17,"_SIDD_MASKED_POSITIVE_POLARITY",E,"Do not negate results before the end of the string",N,N],[17,"_SIDD_MASKED_NEGATIVE_POLARITY",E,"Negate results only before the end of the string",N,N],[17,"_SIDD_LEAST_SIGNIFICANT",E,"Index only: return the least significant bit (Default)",N,N],[17,"_SIDD_MOST_SIGNIFICANT",E,"Index only: return the most significant bit",N,N],[17,"_SIDD_BIT_MASK",E,"Mask only: return the bit mask",N,N],[17,"_SIDD_UNIT_MASK",E,"Mask only: return the byte mask",N,N],[17,"_CMP_EQ_OQ",E,"Equal (ordered, non-signaling)",N,N],[17,"_CMP_LT_OS",E,"Less-than (ordered, signaling)",N,N],[17,"_CMP_LE_OS",E,"Less-than-or-equal (ordered, signaling)",N,N],[17,"_CMP_UNORD_Q",E,"Unordered (non-signaling)",N,N],[17,"_CMP_NEQ_UQ",E,"Not-equal (unordered, non-signaling)",N,N],[17,"_CMP_NLT_US",E,"Not-less-than (unordered, signaling)",N,N],[17,"_CMP_NLE_US",E,"Not-less-than-or-equal (unordered, signaling)",N,N],[17,"_CMP_ORD_Q",E,"Ordered (non-signaling)",N,N],[17,"_CMP_EQ_UQ",E,"Equal (unordered, non-signaling)",N,N],[17,"_CMP_NGE_US",E,"Not-greater-than-or-equal (unordered, signaling)",N,N],[17,"_CMP_NGT_US",E,"Not-greater-than (unordered, signaling)",N,N],[17,"_CMP_FALSE_OQ",E,"False (ordered, non-signaling)",N,N],[17,"_CMP_NEQ_OQ",E,"Not-equal (ordered, non-signaling)",N,N],[17,"_CMP_GE_OS",E,"Greater-than-or-equal (ordered, signaling)",N,N],[17,"_CMP_GT_OS",E,"Greater-than (ordered, signaling)",N,N],[17,"_CMP_TRUE_UQ",E,"True (unordered, non-signaling)",N,N],[17,"_CMP_EQ_OS",E,"Equal (ordered, signaling)",N,N],[17,"_CMP_LT_OQ",E,"Less-than (ordered, non-signaling)",N,N],[17,"_CMP_LE_OQ",E,"Less-than-or-equal (ordered, non-signaling)",N,N],[17,"_CMP_UNORD_S",E,"Unordered (signaling)",N,N],[17,"_CMP_NEQ_US",E,"Not-equal (unordered, signaling)",N,N],[17,"_CMP_NLT_UQ",E,"Not-less-than (unordered, non-signaling)",N,N],[17,"_CMP_NLE_UQ",E,"Not-less-than-or-equal (unordered, non-signaling)",N,N],[17,"_CMP_ORD_S",E,"Ordered (signaling)",N,N],[17,"_CMP_EQ_US",E,"Equal (unordered, signaling)",N,N],[17,"_CMP_NGE_UQ",E,"Not-greater-than-or-equal (unordered, non-signaling)",N,N],[17,"_CMP_NGT_UQ",E,"Not-greater-than (unordered, non-signaling)",N,N],[17,"_CMP_FALSE_OS",E,"False (ordered, signaling)",N,N],[17,"_CMP_NEQ_OS",E,"Not-equal (ordered, signaling)",N,N],[17,"_CMP_GE_OQ",E,"Greater-than-or-equal (ordered, non-signaling)",N,N],[17,"_CMP_GT_OQ",E,"Greater-than (ordered, non-signaling)",N,N],[17,"_CMP_TRUE_US",E,"True (unordered, signaling)",N,N],[11,r_ne,E,E,1,[[[T]],[T]]],[11,r_he,E,E,1,[[[U]],[r_fe]]],[11,r_ie,E,E,1,[[[r_ge]],[r_fe]]],[11,r_oe,E,E,1,[[[r_ge]],[U]]],[11,r_je,E,E,1,[[[r_ge]],[T]]],[11,r_ke,E,E,1,[[[r_ge]],[T]]],[11,r_le,E,E,1,[[[r_ge]],[r_me]]],[11,r_ne,E,E,2,[[[T]],[T]]],[11,r_he,E,E,2,[[[U]],[r_fe]]],[11,r_ie,E,E,2,[[[r_ge]],[r_fe]]],[11,r_oe,E,E,2,[[[r_ge]],[U]]],[11,r_je,E,E,2,[[[r_ge]],[T]]],[11,r_ke,E,E,2,[[[r_ge]],[T]]],[11,r_le,E,E,2,[[[r_ge]],[r_me]]],[11,r_ne,E,E,3,[[[T]],[T]]],[11,r_he,E,E,3,[[[U]],[r_fe]]],[11,r_ie,E,E,3,[[[r_ge]],[r_fe]]],[11,r_oe,E,E,3,[[[r_ge]],[U]]],[11,r_je,E,E,3,[[[r_ge]],[T]]],[11,r_ke,E,E,3,[[[r_ge]],[T]]],[11,r_le,E,E,3,[[[r_ge]],[r_me]]],[11,r_ne,E,E,4,[[[T]],[T]]],[11,r_he,E,E,4,[[[U]],[r_fe]]],[11,r_ie,E,E,4,[[[r_ge]],[r_fe]]],[11,r_oe,E,E,4,[[[r_ge]],[U]]],[11,r_je,E,E,4,[[[r_ge]],[T]]],[11,r_ke,E,E,4,[[[r_ge]],[T]]],[11,r_le,E,E,4,[[[r_ge]],[r_me]]],[11,r_ne,E,E,5,[[[T]],[T]]],[11,r_he,E,E,5,[[[U]],[r_fe]]],[11,r_ie,E,E,5,[[[r_ge]],[r_fe]]],[11,r_oe,E,E,5,[[[r_ge]],[U]]],[11,r_je,E,E,5,[[[r_ge]],[T]]],[11,r_ke,E,E,5,[[[r_ge]],[T]]],[11,r_le,E,E,5,[[[r_ge]],[r_me]]],[11,r_ne,E,E,6,[[[T]],[T]]],[11,r_he,E,E,6,[[[U]],[r_fe]]],[11,r_ie,E,E,6,[[[r_ge]],[r_fe]]],[11,r_oe,E,E,6,[[[r_ge]],[U]]],[11,r_je,E,E,6,[[[r_ge]],[T]]],[11,r_ke,E,E,6,[[[r_ge]],[T]]],[11,r_le,E,E,6,[[[r_ge]],[r_me]]],[11,r_ne,E,E,7,[[[T]],[T]]],[11,r_he,E,E,7,[[[U]],[r_fe]]],[11,r_ie,E,E,7,[[[r_ge]],[r_fe]]],[11,r_oe,E,E,7,[[[r_ge]],[U]]],[11,r_je,E,E,7,[[[r_ge]],[T]]],[11,r_ke,E,E,7,[[[r_ge]],[T]]],[11,r_le,E,E,7,[[[r_ge]],[r_me]]],[11,r_ne,E,E,8,[[[T]],[T]]],[11,r_he,E,E,8,[[[U]],[r_fe]]],[11,r_ie,E,E,8,[[[r_ge]],[r_fe]]],[11,r_oe,E,E,8,[[[r_ge]],[U]]],[11,r_je,E,E,8,[[[r_ge]],[T]]],[11,r_ke,E,E,8,[[[r_ge]],[T]]],[11,r_le,E,E,8,[[[r_ge]],[r_me]]],[11,r_ne,E,E,9,[[[T]],[T]]],[11,r_he,E,E,9,[[[U]],[r_fe]]],[11,r_ie,E,E,9,[[[r_ge]],[r_fe]]],[11,r_oe,E,E,9,[[[r_ge]],[U]]],[11,r_je,E,E,9,[[[r_ge]],[T]]],[11,r_ke,E,E,9,[[[r_ge]],[T]]],[11,r_le,E,E,9,[[[r_ge]],[r_me]]],[11,r_ne,E,E,10,[[[T]],[T]]],[11,r_he,E,E,10,[[[U]],[r_fe]]],[11,r_ie,E,E,10,[[[r_ge]],[r_fe]]],[11,r_oe,E,E,10,[[[r_ge]],[U]]],[11,r_je,E,E,10,[[[r_ge]],[T]]],[11,r_ke,E,E,10,[[[r_ge]],[T]]],[11,r_le,E,E,10,[[[r_ge]],[r_me]]],[11,r_ne,E,E,0,[[[T]],[T]]],[11,r_he,E,E,0,[[[U]],[r_fe]]],[11,r_ie,E,E,0,[[[r_ge]],[r_fe]]],[11,r_oe,E,E,0,[[[r_ge]],[U]]],[11,r_je,E,E,0,[[[r_ge]],[T]]],[11,r_ke,E,E,0,[[[r_ge]],[T]]],[11,r_le,E,E,0,[[[r_ge]],[r_me]]],[11,r_qe,E,E,1,[[[r_ge],[r_pe]],[r_fe]]],[11,r_qe,E,E,2,[[[r_ge],[r_pe]],[r_fe]]],[11,r_qe,E,E,3,[[[r_ge],[r_pe]],[r_fe]]],[11,r_qe,E,E,4,[[[r_ge],[r_pe]],[r_fe]]],[11,r_qe,E,E,5,[[[r_ge],[r_pe]],[r_fe]]],[11,r_qe,E,E,6,[[[r_ge],[r_pe]],[r_fe]]],[11,r_qe,E,E,7,[[[r_ge],[r_pe]],[r_fe]]],[11,r_qe,E,E,8,[[[r_ge],[r_pe]],[r_fe]]],[11,r_qe,E,E,9,[[[r_ge],[r_pe]],[r_fe]]],[11,r_qe,E,E,10,[[[r_ge],[r_pe]],[r_fe]]],[11,r_qe,E,E,0,[[[r_ge],[r_pe]],[r_fe]]],[11,"eq",E,E,0,[[[r_ge],[r_ca]],[r_re]]],[11,"ne",E,E,0,[[[r_ge],[r_ca]],[r_re]]],[11,"cmp",E,E,0,[[[r_ge],[r_ca]],[r_se]]],[11,"partial_cmp",E,E,0,[[[r_ge],[r_ca]],["option",[r_se]]]],[11,"lt",E,E,0,[[[r_ge],[r_ca]],[r_re]]],[11,"le",E,E,0,[[[r_ge],[r_ca]],[r_re]]],[11,"gt",E,E,0,[[[r_ge],[r_ca]],[r_re]]],[11,"ge",E,E,0,[[[r_ge],[r_ca]],[r_re]]],[11,r_te,E,E,1,[[[r_ge]],[r_sa]]],[11,r_te,E,E,2,[[[r_ge]],[r_Ka]]],[11,r_te,E,E,3,[[[r_ge]],[r_fa]]],[11,r_te,E,E,4,[[[r_ge]],[r_Wa]]],[11,r_te,E,E,5,[[[r_ge]],[r_3b]]],[11,r_te,E,E,6,[[[r_ge]],[r_Rb]]],[11,r_te,E,E,7,[[[r_ge]],[r_Qb]]],[11,r_te,E,E,8,[[[r_ge]],[r_Rd]]],[11,r_te,E,E,9,[[[r_ge]],[r_ue]]],[11,r_te,E,E,10,[[[r_ge]],[r_ve]]],[11,r_te,E,E,0,[[[r_ge]],[r_ca]]]],"p":[[3,r_we],[3,r_sa],[3,r_Ka],[3,r_fa],[3,r_Wa],[3,r_3b],[3,r_Rb],[3,r_Qb],[3,r_Rd],[3,r_ue],[3,r_ve]]};
initSearch(searchIndex);addSearchOptions(searchIndex);